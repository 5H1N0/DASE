[
["supervised-classification.html", "Chapter 8 Supervised Classification 8.1 Classification Trees 8.2 Rules 8.3 Distanced-based Methods 8.4 Neural Networks 8.5 Support Vector Machine 8.6 Probabilistic Methods 8.7 Linear Discriminant Analysis (LDA) 8.8 Binary Logistic Regression (BLR) 8.9 The caret package", " Chapter 8 Supervised Classification A classification problem can be defined as the induction, from a dataset \\(\\cal D\\), of a classification function \\(\\psi\\) that, given the attribute vector of an instance/example, returns a class \\({c}\\). A regression problem, on the other hand, returns an numeric value. Dataset, \\(\\cal D\\), is typically composed of \\(n\\) attributes and a class attribute \\(C\\). \\(Att_1\\) … \\(Att_n\\) \\(Class\\) \\(a_{11}\\) … \\(a_{1n}\\) \\(c_1\\) \\(a_{21}\\) … \\(a_{2n}\\) \\(c_2\\) … … … … \\(a_{m1}\\) … \\(a_{mn}\\) \\(c_m\\) Columns are usually called attributes or features. Typically, there is a class attribute, which can be numeric or discrete. When the class is numeric, it is a regression problem. With discrete values, we can talk about binary classification or multiclass (multinomial classification) when we have more than three values. There are variants such multi-label classification (we will cover these in the advanced models section). Once we learn a model, new instances are classified. As shown in the next figure. Supervised Classification We have multiple types of models such as classification trees, rules, neural networks, and probabilistic classifiers that can be used to classify instances. Fernandez et al provide an extensive comparison of 176 classifiers using the UCI dataset (Fernández-Delgado et al. 2014). We will show the use of different classification techniques in the problem of defect prediction as running example. In this example,the different datasets are composed of classical metrics (Halstead or McCabe metrics) based on counts of operators/operands and like or object-oriented metrics (e.g. Chidamber and Kemerer) and the class attribute indicating whether the module or class was defective. 8.1 Classification Trees There are several packages for inducing classification trees, for example with the party package (recursive partitioning): library(foreign) # To load arff file library(party) # Build a decision tree library(caret) jm1 &lt;- read.arff(&quot;./datasets/defectPred/D1/JM1.arff&quot;) str(jm1) ## &#39;data.frame&#39;: 9593 obs. of 22 variables: ## $ LOC_BLANK : num 447 37 11 106 101 67 105 18 39 143 ... ## $ BRANCH_COUNT : num 826 29 405 240 464 187 344 47 163 67 ... ## $ LOC_CODE_AND_COMMENT : num 12 8 0 7 11 4 9 0 1 7 ... ## $ LOC_COMMENTS : num 157 42 17 344 75 1 40 10 6 49 ... ## $ CYCLOMATIC_COMPLEXITY: num 470 19 404 127 263 94 207 24 94 34 ... ## $ DESIGN_COMPLEXITY : num 385 19 2 105 256 63 171 13 67 25 ... ## $ ESSENTIAL_COMPLEXITY : num 113 6 1 33 140 27 58 1 3 1 ... ## $ LOC_EXECUTABLE : num 2824 133 814 952 1339 ... ## $ HALSTEAD_CONTENT : num 210 108 101 218 106 ... ## $ HALSTEAD_DIFFICULTY : num 384.4 46.3 206 215.2 337.4 ... ## $ HALSTEAD_EFFORT : num 31079782 232044 4294926 10100867 12120796 ... ## $ HALSTEAD_ERROR_EST : num 26.95 1.67 6.95 15.65 11.98 ... ## $ HALSTEAD_LENGTH : num 8441 685 2033 5669 4308 ... ## $ HALSTEAD_LEVEL : num 0 0.02 0 0 0 0.02 0 0.03 0.01 0.02 ... ## $ HALSTEAD_PROG_TIME : num 1726655 12891 238607 561159 673378 ... ## $ HALSTEAD_VOLUME : num 80843 5009 20848 46944 35928 ... ## $ NUM_OPERANDS : num 3021 295 813 2301 1556 ... ## $ NUM_OPERATORS : num 5420 390 1220 3368 2752 ... ## $ NUM_UNIQUE_OPERANDS : num 609 121 811 262 226 167 279 47 117 355 ... ## $ NUM_UNIQUE_OPERATORS : num 155 38 411 49 98 27 105 18 52 23 ... ## $ LOC_TOTAL : num 3442 222 844 1411 1532 ... ## $ Defective : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 2 2 2 2 2 2 2 2 1 2 ... # Stratified partition (training and test sets) set.seed(1234) inTrain &lt;- createDataPartition(y=jm1$Defective,p=.60,list=FALSE) jm1.train &lt;- jm1[inTrain,] jm1.test &lt;- jm1[-inTrain,] jm1.formula &lt;- jm1$Defective ~ . # formula approach: defect as dependent variable and the rest as independent variables jm1.ctree &lt;- ctree(jm1.formula, data = jm1.train) # predict on test data pred &lt;- predict(jm1.ctree, newdata = jm1.test) # check prediction result table(pred, jm1.test$Defective) ## ## pred N Y ## N 82 3 ## Y 3051 700 plot(jm1.ctree) Using the C50 package, there are two ways, specifying train and testing library(C50) require(utils) # c50t &lt;- C5.0(jm1.train[,-ncol(jm1.train)], jm1.train[,ncol(jm1.train)]) c50t2 &lt;- C5.0(Defective ~ ., jm1.train) summary(c50t) plot(c50t) c50tPred &lt;- predict(c50t, jm1.train) # table(c50tPred, jm1.train$Defective) Using the ‘rpart’ package # Using the &#39;rpart&#39; package library(rpart) jm1.rpart &lt;- rpart(Defective ~ ., data=jm1.train, parms = list(prior = c(.65,.35), split = &quot;information&quot;)) # par(mfrow = c(1,2), xpd = NA) plot(jm1.rpart) text(jm1.rpart, use.n = TRUE) jm1.rpart ## n= 5757 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 5757 2014.95000 N (0.6500000 0.3500000) ## 2) LOC_TOTAL&lt; 42.5 4309 1032.28000 N (0.7439560 0.2560440) * ## 3) LOC_TOTAL&gt;=42.5 1448 742.67870 Y (0.4304514 0.5695486) ## 6) NUM_UNIQUE_OPERANDS&lt; 59.5 1206 655.11750 Y (0.4726955 0.5273045) ## 12) LOC_BLANK&lt; 7.5 449 209.89060 N (0.5624895 0.4375105) ## 24) LOC_CODE_AND_COMMENT&lt; 3.5 412 171.72870 N (0.5988063 0.4011937) * ## 25) LOC_CODE_AND_COMMENT&gt;=3.5 37 13.53220 Y (0.2617743 0.7382257) * ## 13) LOC_BLANK&gt;=7.5 757 385.26960 Y (0.4251579 0.5748421) ## 26) HALSTEAD_DIFFICULTY&gt;=64.45 55 13.35668 N (0.7409751 0.2590249) * ## 27) HALSTEAD_DIFFICULTY&lt; 64.45 702 347.06100 Y (0.4061023 0.5938977) * ## 7) NUM_UNIQUE_OPERANDS&gt;=59.5 242 87.56126 Y (0.2579656 0.7420344) * library(rpart.plot) # asRules(jm1.rpart) # fancyRpartPlot(jm1.rpart) 8.2 Rules C5 Rules library(C50) c50r &lt;- C5.0(jm1.train[,-ncol(jm1.train)], jm1.train[,ncol(jm1.train)], rules = TRUE) summary(c50r) ## ## Call: ## C5.0.default(x = jm1.train[, -ncol(jm1.train)], y = ## jm1.train[, ncol(jm1.train)], rules = TRUE) ## ## ## C5.0 [Release 2.07 GPL Edition] Sat Jul 1 13:51:45 2017 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 5757 cases (22 attributes) from undefined.data ## ## Rules: ## ## Rule 1: (5512/923, lift 1.0) ## CYCLOMATIC_COMPLEXITY &lt;= 67 ## NUM_UNIQUE_OPERANDS &lt;= 59 ## -&gt; class N [0.832] ## ## Rule 2: (11/1, lift 4.6) ## LOC_BLANK &gt; 3 ## DESIGN_COMPLEXITY &gt; 3 ## LOC_EXECUTABLE &gt; 31 ## LOC_EXECUTABLE &lt;= 33 ## LOC_TOTAL &lt;= 42 ## -&gt; class Y [0.846] ## ## Rule 3: (25/5, lift 4.2) ## CYCLOMATIC_COMPLEXITY &gt; 67 ## -&gt; class Y [0.778] ## ## Rule 4: (242/110, lift 3.0) ## NUM_UNIQUE_OPERANDS &gt; 59 ## LOC_TOTAL &gt; 42 ## -&gt; class Y [0.545] ## ## Default class: N ## ## ## Evaluation on training data (5757 cases): ## ## Rules ## ---------------- ## No Errors ## ## 4 1025(17.8%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 4589 112 (a): class N ## 913 143 (b): class Y ## ## ## Attribute usage: ## ## 99.95% NUM_UNIQUE_OPERANDS ## 96.18% CYCLOMATIC_COMPLEXITY ## 4.39% LOC_TOTAL ## 0.19% LOC_BLANK ## 0.19% DESIGN_COMPLEXITY ## 0.19% LOC_EXECUTABLE ## ## ## Time: 0.2 secs # c50rPred &lt;- predict(c50r, jm1.train) # table(c50rPred, jm1.train$Defective) 8.3 Distanced-based Methods In this case, there is no model as such. Given a new instance to classify, this approach finds the closest \\(k\\)-neighbours to the given instance. (Source: Wikipedia - https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) library(class) m1 &lt;- knn(train=jm1.train[,-22], test=jm1.test[,-22], cl=jm1.train[,22], k=3) table(jm1.test[,22],m1) ## m1 ## N Y ## N 2827 306 ## Y 553 150 8.4 Neural Networks Neural Networks Neural Networks 8.5 Support Vector Machine (Source: wikipedia https://en.wikipedia.org/wiki/Support_vector_machine) 8.6 Probabilistic Methods 8.6.1 Naive Bayes Probabilistic graphical model assigning a probability to each possible outcome \\(p(C_k, x_1,\\ldots,x_n)\\) Naive Bayes Using the klaR package with caret: library(caret) library(klaR) ## Loading required package: MASS model &lt;- NaiveBayes(Defective ~ ., data = jm1.train) predictions &lt;- predict(model, jm1.test[,-22]) confusionMatrix(predictions$class, jm1.test$Defective) ## Confusion Matrix and Statistics ## ## Reference ## Prediction N Y ## N 2990 548 ## Y 143 155 ## ## Accuracy : 0.8199 ## 95% CI : (0.8073, 0.8319) ## No Information Rate : 0.8167 ## P-Value [Acc &gt; NIR] : 0.3168 ## ## Kappa : 0.2251 ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Sensitivity : 0.9544 ## Specificity : 0.2205 ## Pos Pred Value : 0.8451 ## Neg Pred Value : 0.5201 ## Prevalence : 0.8167 ## Detection Rate : 0.7795 ## Detection Prevalence : 0.9223 ## Balanced Accuracy : 0.5874 ## ## &#39;Positive&#39; Class : N ## Using the e1071 package: library (e1071) n1 &lt;-naiveBayes(jm1.train$Defective ~ ., data=jm1.train) # Show first 3 results using &#39;class&#39; head(predict(n1,jm1.test, type = c(&quot;class&quot;)),3) # class by default ## [1] N Y Y ## Levels: N Y # Show first 3 results using &#39;raw&#39; head(predict(n1,jm1.test, type = c(&quot;raw&quot;)),3) ## N Y ## [1,] 1.0000000000 0.0000000 ## [2,] 0.0000000000 1.0000000 ## [3,] 0.0007540951 0.9992459 There are other variants such as TAN and KDB that do not assume the independece condition allowin us more complex structures. Naive Bayes Naive Bayes A comprehensice comparison of 8.7 Linear Discriminant Analysis (LDA) One classical approach to classification is Linear Discriminant Analysis (LDA), a generalization of Fisher’s linear discriminant, as a method used to find a linear combination of features to separate two or more classes. ldaModel &lt;- train (Defective ~ ., data=jm1.train, method=&quot;lda&quot;, preProc=c(&quot;center&quot;,&quot;scale&quot;)) ldaModel ## Linear Discriminant Analysis ## ## 5757 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 5757, 5757, 5757, 5757, 5757, 5757, ... ## Resampling results: ## ## Accuracy Kappa ## 0.8184746 0.1371754 We can observe that we are training our model using Defective ~ . as a formula were Defective is the class variable separed by ~ and the ´.´ means the rest of the variables. Also, we are using a filter for the training data to (preProc) to center and scale. Also, as stated in the documentation about the train method : &gt; http://topepo.github.io/caret/training.html ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,repeats=3) ldaModel &lt;- train (Defective ~ ., data=jm1.train, method=&quot;lda&quot;, trControl=ctrl, preProc=c(&quot;center&quot;,&quot;scale&quot;)) ldaModel ## Linear Discriminant Analysis ## ## 5757 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 5182, 5181, 5181, 5181, 5181, 5181, ... ## Resampling results: ## ## Accuracy Kappa ## 0.8167443 0.1308431 Instead of accuracy we can activate other metrics using summaryFunction=twoClassSummary such as ROC, sensitivity and specificity. To do so, we also need to speficy classProbs=TRUE. ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,repeats=3, classProbs=TRUE, summaryFunction=twoClassSummary) ldaModel3xcv10 &lt;- train (Defective ~ ., data=jm1.train, method=&quot;lda&quot;, trControl=ctrl, preProc=c(&quot;center&quot;,&quot;scale&quot;)) ldaModel3xcv10 ## Linear Discriminant Analysis ## ## 5757 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 5181, 5182, 5180, 5181, 5182, 5181, ... ## Resampling results: ## ## ROC Sens Spec ## 0.7073554 0.9731275 0.1189847 Most methods have parameters that need to be optimised and that is one of the plsFit3x10cv &lt;- train (Defective ~ ., data=jm1.train, method=&quot;pls&quot;, trControl=trainControl(classProbs=TRUE), metric=&quot;ROC&quot;, preProc=c(&quot;center&quot;,&quot;scale&quot;)) plsFit3x10cv ## Partial Least Squares ## ## 5757 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 5757, 5757, 5757, 5757, 5757, 5757, ... ## Resampling results across tuning parameters: ## ## ncomp Accuracy Kappa ## 1 0.8204925 0.06414865 ## 2 0.8196635 0.08261024 ## 3 0.8200588 0.08777031 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was ncomp = 1. plot(plsFit3x10cv) The parameter tuneLength allow us to specify the number values per parameter to consider. plsFit3x10cv &lt;- train (Defective ~ ., data=jm1.train, method=&quot;pls&quot;, trControl=ctrl, metric=&quot;ROC&quot;, tuneLength=5, preProc=c(&quot;center&quot;,&quot;scale&quot;)) plsFit3x10cv ## Partial Least Squares ## ## 5757 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 5182, 5182, 5181, 5181, 5181, 5181, ... ## Resampling results across tuning parameters: ## ## ncomp ROC Sens Spec ## 1 0.6930018 0.9951784 0.04482480 ## 2 0.6989680 0.9904273 0.06441450 ## 3 0.7019246 0.9892927 0.06441450 ## 4 0.7044831 0.9887963 0.06693621 ## 5 0.7052068 0.9884417 0.06756813 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was ncomp = 5. plot(plsFit3x10cv) Finally to predict new cases, caret will use the best classfier obtained for prediction. plsProbs &lt;- predict(plsFit3x10cv, newdata = jm1.test, type = &quot;prob&quot;) plsClasses &lt;- predict(plsFit3x10cv, newdata = jm1.test, type = &quot;raw&quot;) confusionMatrix(data=plsClasses,jm1.test$Defective) ## Confusion Matrix and Statistics ## ## Reference ## Prediction N Y ## N 3108 643 ## Y 25 60 ## ## Accuracy : 0.8259 ## 95% CI : (0.8135, 0.8377) ## No Information Rate : 0.8167 ## P-Value [Acc &gt; NIR] : 0.07427 ## ## Kappa : 0.1174 ## Mcnemar&#39;s Test P-Value : &lt; 2e-16 ## ## Sensitivity : 0.99202 ## Specificity : 0.08535 ## Pos Pred Value : 0.82858 ## Neg Pred Value : 0.70588 ## Prevalence : 0.81674 ## Detection Rate : 0.81022 ## Detection Prevalence : 0.97784 ## Balanced Accuracy : 0.53868 ## ## &#39;Positive&#39; Class : N ## 8.7.1 Predicting the number of defects (numerical class) From the Bug Prediction Repository (BPR) http://bug.inf.usi.ch/download.php Some datasets contain CK and other 11 object-oriented metrics for the last version of the system plus categorized (with severity and priority) post-release defects. Using such dataset: jdt &lt;- read.csv(&quot;./datasets/defectPred/BPD/single-version-ck-oo-EclipseJDTCore.csv&quot;, sep=&quot;;&quot;) # We just use the number of bugs, so we removed others jdt$classname &lt;- NULL jdt$nonTrivialBugs &lt;- NULL jdt$majorBugs &lt;- NULL jdt$minorBugs &lt;- NULL jdt$criticalBugs &lt;- NULL jdt$highPriorityBugs &lt;- NULL jdt$X &lt;- NULL # Caret library(caret) # Split data into training and test datasets set.seed(1) inTrain &lt;- createDataPartition(y=jdt$bugs,p=.8,list=FALSE) jdt.train &lt;- jdt[inTrain,] jdt.test &lt;- jdt[-inTrain,] ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,repeats=3) glmModel &lt;- train (bugs ~ ., data=jdt.train, method=&quot;glm&quot;, trControl=ctrl, preProc=c(&quot;center&quot;,&quot;scale&quot;)) glmModel ## Generalized Linear Model ## ## 798 samples ## 17 predictors ## ## Pre-processing: centered (17), scaled (17) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 718, 718, 718, 718, 719, 718, ... ## Resampling results: ## ## RMSE Rsquared ## 0.8411011 0.3855316 Others such as Elasticnet: glmnetModel &lt;- train (bugs ~ ., data=jdt.train, method=&quot;glmnet&quot;, trControl=ctrl, preProc=c(&quot;center&quot;,&quot;scale&quot;)) ## Loading required package: glmnet ## Loading required package: Matrix ## Loading required package: foreach ## Loaded glmnet 2.0-10 glmnetModel ## glmnet ## ## 798 samples ## 17 predictors ## ## Pre-processing: centered (17), scaled (17) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 718, 718, 718, 718, 718, 718, ... ## Resampling results across tuning parameters: ## ## alpha lambda RMSE Rsquared ## 0.10 0.001202348 0.8127568 0.3411090 ## 0.10 0.012023480 0.8183111 0.3344713 ## 0.10 0.120234797 0.8077544 0.3396316 ## 0.55 0.001202348 0.8119513 0.3412175 ## 0.55 0.012023480 0.8227484 0.3268770 ## 0.55 0.120234797 0.8117291 0.3473930 ## 1.00 0.001202348 0.8116078 0.3407650 ## 1.00 0.012023480 0.8189354 0.3309040 ## 1.00 0.120234797 0.8167381 0.3445179 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were alpha = 0.1 and lambda ## = 0.1202348. 8.8 Binary Logistic Regression (BLR) Binary Logistic Regression (BLR) can models fault-proneness as follows \\[fp(X) = \\frac{e^{logit()}}{1 + e^{logit(X)}}\\] where the simplest form for logit is: \\(logit(X) = c_{0} + c_{1}X\\) jdt &lt;- read.csv(&quot;./datasets/defectPred/BPD/single-version-ck-oo-EclipseJDTCore.csv&quot;, sep=&quot;;&quot;) # Caret library(caret) # Convert the response variable into a boolean variable (0/1) jdt$bugs[jdt$bugs&gt;=1]&lt;-1 cbo &lt;- jdt$cbo bugs &lt;- jdt$bugs # Split data into training and test datasets jdt2 = data.frame(cbo, bugs) inTrain &lt;- createDataPartition(y=jdt2$bugs,p=.8,list=FALSE) jdtTrain &lt;- jdt2[inTrain,] jdtTest &lt;- jdt2[-inTrain,] BLR models fault-proneness are as follows \\(fp(X) = \\frac{e^{logit()}}{1 + e^{logit(X)}}\\) where the simplest form for logit is \\(logit(X) = c_{0} + c_{1}X\\) # logit regression # glmLogit &lt;- train (bugs ~ ., data=jdt.train, method=&quot;glm&quot;, family=binomial(link = logit)) glmLogit &lt;- glm (bugs ~ ., data=jdtTrain, family=binomial(link = logit)) summary(glmLogit) ## ## Call: ## glm(formula = bugs ~ ., family = binomial(link = logit), data = jdtTrain) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.5734 -0.6125 -0.5378 -0.4968 2.0992 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.086378 0.134620 -15.498 &lt; 2e-16 *** ## cbo 0.056462 0.007045 8.014 1.11e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 831.84 on 797 degrees of freedom ## Residual deviance: 725.93 on 796 degrees of freedom ## AIC: 729.93 ## ## Number of Fisher Scoring iterations: 5 Predict a single point: newData = data.frame(cbo = 3) predict(glmLogit, newData, type = &quot;response&quot;) ## 1 ## 0.1281974 Draw the results, modified from: http://www.shizukalab.com/toolkits/plotting-logistic-regression-in-r results &lt;- predict(glmLogit, jdtTest, type = &quot;response&quot;) range(jdtTrain$cbo) ## [1] 0 156 range(results) ## [1] 0.1104278 0.9840854 plot(jdt2$cbo,jdt2$bugs) curve(predict(glmLogit, data.frame(cbo=x), type = &quot;response&quot;),add=TRUE) # points(jdtTrain$cbo,fitted(glmLogit)) Another type of graph: library(popbio) ## ## Attaching package: &#39;popbio&#39; ## The following object is masked from &#39;package:caret&#39;: ## ## sensitivity logi.hist.plot(jdt2$cbo,jdt2$bugs,boxp=FALSE,type=&quot;hist&quot;,col=&quot;gray&quot;) 8.9 The caret package There are hundreds of packages to perform classification task in R, but many of those can be used throught the ‘caret’ package which helps with many of the data mining process task as described next. The caret packagehttp://topepo.github.io/caret/ provides a unified interface for modeling and prediction with around 150 different models with tools for: data splitting pre-processing feature selection model tuning using resampling variable importance estimation, etc. Website: http://caret.r-forge.r-project.org JSS Paper: www.jstatsoft.org/v28/i05/paper Book: Applied Predictive Modeling Placeholder "]
]
