---
output:
  pdf_document: default
  html_document: default
---

# Text Mining Software Engineering Data

In software engineering, there is a lot of information that can be extracted from Software Configuration Management System (SCM), or Bug Tracking Systems such as Bugzilla. 

The main packages for Text Mining are "tm", "wordcloud".
```{r}
# - Install packages

# pckgs_needed <- c("tm", "wordcloud")
# install.packages(pckgs_needed, dependencies = TRUE)
```
  - Importing data. A "Corpus" is a collection of text documents, implemented as VCorpus (corpora are R object held in memory). tm provides several corpus constructors: DirSource, VectorSource, or DataframeSource (getSources()). readerControl of the corpus constructor has to be a list with the named components reader and language
  - Preprocessing 
  - Inspecting and exploring data: Individual documents can be accessed via [[
  - Transformations: Transformations are done via the tm_map() function. tm_map(_____, stripWhitespace)  tm_map(_____, content_transformer(tolower))  tm_map(_____, removeWords, stopwords("english"))  tm_map(tm_map(reuters, stemDocument), stemDocument)
  - Creating Term-Document Matrices: TermDocumentMatrix and DocumentTermMatrix
  - Relationships between terms.
    findFreqTerms(_____, anumber)
    findAssocs(_dtm____, "aterm", anumbercorrelation)
    A dictionary is a (multi-)set of strings. It is often used to denote relevant terms in text mining.
  - Clustering
    




## Example of classifying bugs from Bugzilla 


Bugzilla is Issue Tracking System that allow us to maintain and track the evolution of a project.
The following example shows how to work with entries from Bugzilla. It is assumed that the data has been extracted and we have the records in a flat file (this can be done using Web crawlers or directly using the SQL database).


```{r message=FALSE, warning=FALSE}
library(foreign)
# path_name <- file.path("C:", "datasets", "textMining")
# path_name
# dir(path_name)

d <- read.arff("./datasets/textMining/compendium.arff")
head(d,5)
```

Creating a Document-Term Matrix (DTM)

```{r message=FALSE, warning=FALSE}
library(tm)

dfcomp <- data.frame(textCol = d$Description)  # , d$Category)

ds <- DataframeSource(dfcomp)
dsc <-Corpus(ds)

# weighting=TfIdf weighting is Tf-Idf
# minWordLength=WL the minimum word length is WL
# minDocFreq=ND each word must appear at least in ND docs

# Other options of DTM
# These are not really needed, if preprocessing has been carried out:
# stemming=TRUE stemming is applied
# stopwords=TRUE stopwords are eliminated
# removeNumbers=TRUE numbers are eliminated


dtm<- DocumentTermMatrix(dsc, control = list(weighting = weightTfIdf, minDocFreq=3, stopwords = TRUE, removeNumbers = TRUE))

# dim(dtm)
# inspect(dtm) #[1:10,1:10])

# dtm.70=removeSparseTerms(dtm,sparse=0.7)
# dtm.70 # or dim(dtm.70)
# note that the term-document matrix needs to be transformed (casted)
# to a matrix form in the following barplot command

dtm.90=removeSparseTerms(dtm,sparse=0.9)

barplot(as.matrix(dtm),xlab="terms",ylab="number of occurrences",main="Most frequent terms (sparseness=0.9)")
```

As data frame:

```{r results='hide'}
#dtmdf <- as.data.frame(dtm.90)
dtmdf <- as.data.frame(inspect(dtm.90))
# rownames(dtm)<- 1:nrow(dtm)


class <- d$Category
dtmdf <- cbind(dtmdf,class)
```

Now, we can explore things such as "which words are associated with "feature"?"
```{r}
# which words are associated with "feature"?
findAssocs(dtm, 'feature', 0.40)
```

And find frequent terms.

```{r}
findFreqTerms(dtm,5)
```

Use any classifier now:

```{r}
library(caret)
library(randomForest)

inTraining <- createDataPartition(dtmdf$class, p = .75, list = FALSE)
training <- dtmdf[ inTraining,]
testing  <- dtmdf[-inTraining,]

fitControl <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 5)


gbmFit1 <- train(class ~ ., data = training,
                 method = "gbm",
                 trControl = fitControl,
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE)

gbmFit1

trellis.par.set(caretTheme())
plot(gbmFit1)

trellis.par.set(caretTheme())
plot(gbmFit1, metric = "Kappa")

head(predict(gbmFit1, testing, type = "prob"))

confusionMatrix(testing$class, predict(gbmFit1, testing))

```


And finally, a word cloud as an example that appears everywhere these days.

```{r}
library(wordcloud)
# calculate the frequency of words and sort in descending order.
wordFreqs=sort(colSums(as.matrix(dtm.90)),decreasing=TRUE)
wordcloud(words=names(wordFreqs),freq=wordFreqs)
```


## Extracting data from Twitter

The hardest bit is to link with Twitter. Using the TwitteR package is explained following this [example](./twitter.Rmd).

## References 
  - Ingo Feinerer, tm: Text Mining Package, 2015. R package version 0.6.2 2015-07-03
