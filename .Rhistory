install.packages("UsingR")
install.packages("caret")
install.packages(c("plyr", "StanHeaders"))
install.packages("party")
install.packages("C50")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("klaR")
install.packages("e1071")
install.packages("fpc")
install.packages("arules")
install.packages("arulesViz")
library(UsingR); data(galton)
par(mfrow=c(1,2))
hist(galton$child,col="blue",breaks=100)
hist(galton$parent,col="blue",breaks=100)
plot(galton$parent,galton$child,pch=1,col="blue", cex=0.4)
lm1 <- lm(galton$child ~ galton$parent)
lines(galton$parent,lm1$fitted,col="red",lwd=3)
plot(galton$parent,lm1$residuals,col="blue",pch=1, cex=0.4)
abline(c(0,0),col="red",lwd=3)
qqnorm(galton$child)
```
## Linear Regression Diagnostics
- Several plots help to evaluate the suitability of the linear regression
+ *Residuals vs fitted*: The residuals should be randomly distributed around the horizontal line representing a residual error of zero; that is, there should not be a distinct trend in the distribution of points.
+ *Standard Q-Q plot*: residual errors are normally distributed
+ *Square root of the standardized residuals vs the fitted value*s: there should be no obvious trend.
+ *Leverage*: measures the importance of each point in determining the regression result. Smaller values means that removing the observation has little effect on the regression result.
library(foreign)
china <- read.arff("../datasets/china.arff")
china <- read.arff("./datasets/china.arff")
china_size <- china$AFP
summary(china_size)
china_effort <- china$Effort
summary(china_effort)
par(mfrow=c(1,2))
linmodel_logchina <- lm(logchina_effort ~ logchina_size)
par(mfrow=c(1,2))
logchina_size = log(china_size)
hist(logchina_size, col="blue", xlab="log Adjusted Function Points", main="Distribution of log AFP")
logchina_effort = log(china_effort)
hist(logchina_effort, col="blue",xlab="Effort", main="Distribution of log Effort")
qqnorm(logchina_size)
qqnorm(logchina_effort)
```
```{r}
linmodel_logchina <- lm(logchina_effort ~ logchina_size)
par(mfrow=c(1,1))
plot(logchina_size, logchina_effort)
abline(linmodel_logchina, lwd=3, col=3)
par(mfrow=c(1,2))
plot(linmodel_logchina, ask = FALSE)
install.packages("rgp")
install.packages("neuralnet")
install.packages("pROC")
library(foreign)
library(caret)
kc1 <- read.arff("../datasets/defectPred/D1/KC1.arff")
kc1 <- read.arff("./datasets/defectPred/D1/KC1.arff")
set.seed(1234)
ind <- sample(2, nrow(kc1), replace = TRUE, prob = c(0.7, 0.3))
kc1.train <- kc1[ind==1, ]
kc1.test <- kc1[ind==2, ]
# create caret trainControl object to control the number of cross-validations performed
objControl <- trainControl(method='cv', number=3, returnResamp='none', summaryFunction = twoClassSummary, classProbs = TRUE)
# run model
objModel <- train(Defective ~ .,
data = kc1.train,
method = 'gbm',
trControl = objControl,
metric = "ROC" #,
#preProc = c("center", "scale")
)
install.packages("glmnet")
vimp <- varImp(objModel, scale=F)
results <- data.frame(row.names(vimp$importance),vimp$importance$Overall)
results$VariableName <- rownames(vimp)
colnames(results) <- c('VariableName','Weight')
results <- results[order(results$Weight),]
results <- results[(results$Weight != 0),]
par(mar=c(5,15,4,2)) # increase y-axis margin.
xx <- barplot(results$Weight, width = 0.85,
main = paste("Variable Importance -",outcomeName), horiz = T,
xlab = "< (-) importance >  < neutral >  < importance (+) >", axes = FALSE,
col = ifelse((results$Weight > 0), 'blue', 'red'))
axis(2, at=xx, labels=results$VariableName, tick=FALSE, las=2, line=-0.3, cex.axis=0.6)
xx <- barplot(results$Weight, width = 0.85,
main = paste("Variable Importance -",outcomeName), horiz = T,
xlab = "< (-) importance >  < neutral >  < importance (+) >", axes = FALSE,
col = ifelse((results$Weight > 0), 'blue', 'red'))
install.packages('vioplot')
violinplot()
