<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 17 Advanced Models | Data Analysis in Software Engineering using R</title>
  <meta name="description" content="DASE Data Analysis in Software Engineering">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 17 Advanced Models | Data Analysis in Software Engineering using R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="DASE Data Analysis in Software Engineering" />
  <meta name="github-repo" content="danrodgar/DASE" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 17 Advanced Models | Data Analysis in Software Engineering using R" />
  
  <meta name="twitter:description" content="DASE Data Analysis in Software Engineering" />
  

<meta name="author" content="Daniel Rodriguez and Javier Dolado">


<meta name="date" content="2018-12-24">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="feature-selection-example.html">
<link rel="next" href="further-classification-models.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis in Software Engineering with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="part"><span><b>I Introduction to the R Language</b></span></li>
<li class="chapter" data-level="1" data-path="r-intro.html"><a href="r-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to R</a><ul>
<li class="chapter" data-level="1.1" data-path="r-intro.html"><a href="r-intro.html#installation"><i class="fa fa-check"></i><b>1.1</b> Installation</a></li>
<li class="chapter" data-level="1.2" data-path="r-intro.html"><a href="r-intro.html#r-and-rstudio"><i class="fa fa-check"></i><b>1.2</b> R and RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="r-intro.html"><a href="r-intro.html#basic-data-types"><i class="fa fa-check"></i><b>1.3</b> Basic Data Types</a><ul>
<li class="chapter" data-level="1.3.1" data-path="r-intro.html"><a href="r-intro.html#mising-values"><i class="fa fa-check"></i><b>1.3.1</b> Mising values</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="r-intro.html"><a href="r-intro.html#vectors"><i class="fa fa-check"></i><b>1.4</b> Vectors</a><ul>
<li class="chapter" data-level="1.4.1" data-path="r-intro.html"><a href="r-intro.html#coercion-for-vectors"><i class="fa fa-check"></i><b>1.4.1</b> Coercion for vectors</a></li>
<li class="chapter" data-level="1.4.2" data-path="r-intro.html"><a href="r-intro.html#vector-arithmetic"><i class="fa fa-check"></i><b>1.4.2</b> Vector arithmetic</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="r-intro.html"><a href="r-intro.html#arrays-and-matrices"><i class="fa fa-check"></i><b>1.5</b> Arrays and Matrices</a></li>
<li class="chapter" data-level="1.6" data-path="r-intro.html"><a href="r-intro.html#factors"><i class="fa fa-check"></i><b>1.6</b> Factors</a></li>
<li class="chapter" data-level="1.7" data-path="r-intro.html"><a href="r-intro.html#lists"><i class="fa fa-check"></i><b>1.7</b> Lists</a></li>
<li class="chapter" data-level="1.8" data-path="r-intro.html"><a href="r-intro.html#data-frames"><i class="fa fa-check"></i><b>1.8</b> Data frames</a></li>
<li class="chapter" data-level="1.9" data-path="r-intro.html"><a href="r-intro.html#reading-data"><i class="fa fa-check"></i><b>1.9</b> Reading Data</a></li>
<li class="chapter" data-level="1.10" data-path="r-intro.html"><a href="r-intro.html#plots"><i class="fa fa-check"></i><b>1.10</b> Plots</a></li>
<li class="chapter" data-level="1.11" data-path="r-intro.html"><a href="r-intro.html#flow-of-control"><i class="fa fa-check"></i><b>1.11</b> Flow of Control</a></li>
<li class="chapter" data-level="1.12" data-path="r-intro.html"><a href="r-intro.html#rattle"><i class="fa fa-check"></i><b>1.12</b> Rattle</a></li>
</ul></li>
<li class="part"><span><b>II Introduction to Data Mining</b></span></li>
<li class="chapter" data-level="2" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><i class="fa fa-check"></i><b>2</b> What is Data Mining / Knowledge Discovery in Databases (KDD)</a><ul>
<li class="chapter" data-level="2.1" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#the-aim-of-data-analysis-and-statistical-learning"><i class="fa fa-check"></i><b>2.1</b> The Aim of Data Analysis and Statistical Learning</a></li>
<li class="chapter" data-level="2.2" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#basic-references"><i class="fa fa-check"></i><b>2.2</b> Basic References</a></li>
<li class="chapter" data-level="2.3" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#data-mining-with-r"><i class="fa fa-check"></i><b>2.3</b> Data Mining with R</a></li>
<li class="chapter" data-level="2.4" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#data-mining-with-weka"><i class="fa fa-check"></i><b>2.4</b> Data Mining with Weka</a></li>
</ul></li>
<li class="part"><span><b>III Data Sources and Metrics and Standards in Software Engineering Defect Prediction</b></span></li>
<li class="chapter" data-level="3" data-path="data-sources-in-software-engineering.html"><a href="data-sources-in-software-engineering.html"><i class="fa fa-check"></i><b>3</b> Data Sources in Software Engineering</a></li>
<li class="chapter" data-level="4" data-path="repositories.html"><a href="repositories.html"><i class="fa fa-check"></i><b>4</b> Repositories</a></li>
<li class="chapter" data-level="5" data-path="open-toolsdashboards-to-extract-data.html"><a href="open-toolsdashboards-to-extract-data.html"><i class="fa fa-check"></i><b>5</b> Open Tools/Dashboards to extract data</a><ul>
<li class="chapter" data-level="5.1" data-path="open-toolsdashboards-to-extract-data.html"><a href="open-toolsdashboards-to-extract-data.html#issues"><i class="fa fa-check"></i><b>5.1</b> Issues</a></li>
<li class="chapter" data-level="5.2" data-path="open-toolsdashboards-to-extract-data.html"><a href="open-toolsdashboards-to-extract-data.html#effort-estimation-data-in-software-engineering"><i class="fa fa-check"></i><b>5.2</b> Effort Estimation Data in Software Engineering</a></li>
</ul></li>
<li class="part"><span><b>IV Exploratory and Descriptive Data analysis</b></span></li>
<li class="chapter" data-level="6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>6</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.1</b> Descriptive statistics</a></li>
<li class="chapter" data-level="6.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#basic-plots"><i class="fa fa-check"></i><b>6.2</b> Basic Plots</a></li>
<li class="chapter" data-level="6.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#normality"><i class="fa fa-check"></i><b>6.3</b> Normality</a></li>
<li class="chapter" data-level="6.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#using-a-running-example-to-visualise-the-different-plots"><i class="fa fa-check"></i><b>6.4</b> Using a running Example to visualise the different plots</a><ul>
<li class="chapter" data-level="6.4.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#example-with-the-china-dataset-from-the-tera-promise-repository"><i class="fa fa-check"></i><b>6.4.1</b> Example with the China dataset (from the tera-Promise Repository)</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#correlation"><i class="fa fa-check"></i><b>6.5</b> Correlation</a></li>
<li class="chapter" data-level="6.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#confidence-intervals.-bootstrap"><i class="fa fa-check"></i><b>6.6</b> Confidence Intervals. Bootstrap</a></li>
<li class="chapter" data-level="6.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#nonparametric-bootstrap"><i class="fa fa-check"></i><b>6.7</b> Nonparametric Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classical-hypothesis-testing.html"><a href="classical-hypothesis-testing.html"><i class="fa fa-check"></i><b>7</b> Classical Hypothesis Testing</a><ul>
<li class="chapter" data-level="7.1" data-path="classical-hypothesis-testing.html"><a href="classical-hypothesis-testing.html#p-values"><i class="fa fa-check"></i><b>7.1</b> p-values</a></li>
</ul></li>
<li class="part"><span><b>V Preprocessing</b></span></li>
<li class="chapter" data-level="8" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>8</b> Preprocessing</a><ul>
<li class="chapter" data-level="8.1" data-path="preprocessing.html"><a href="preprocessing.html#data"><i class="fa fa-check"></i><b>8.1</b> Data</a></li>
<li class="chapter" data-level="8.2" data-path="preprocessing.html"><a href="preprocessing.html#missing-values"><i class="fa fa-check"></i><b>8.2</b> Missing values</a></li>
<li class="chapter" data-level="8.3" data-path="preprocessing.html"><a href="preprocessing.html#noise"><i class="fa fa-check"></i><b>8.3</b> Noise</a></li>
<li class="chapter" data-level="8.4" data-path="preprocessing.html"><a href="preprocessing.html#outliers"><i class="fa fa-check"></i><b>8.4</b> Outliers</a></li>
<li class="chapter" data-level="8.5" data-path="preprocessing.html"><a href="preprocessing.html#feature-selection"><i class="fa fa-check"></i><b>8.5</b> Feature selection</a><ul>
<li class="chapter" data-level="8.5.1" data-path="preprocessing.html"><a href="preprocessing.html#fselector-package-in-r"><i class="fa fa-check"></i><b>8.5.1</b> FSelector package in R</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="preprocessing.html"><a href="preprocessing.html#instance-selection"><i class="fa fa-check"></i><b>8.6</b> Instance selection</a></li>
<li class="chapter" data-level="8.7" data-path="preprocessing.html"><a href="preprocessing.html#discretization"><i class="fa fa-check"></i><b>8.7</b> Discretization</a></li>
<li class="chapter" data-level="8.8" data-path="preprocessing.html"><a href="preprocessing.html#correlation-coefficient-and-covariance-for-numeric-data"><i class="fa fa-check"></i><b>8.8</b> Correlation Coefficient and Covariance for Numeric Data</a></li>
<li class="chapter" data-level="8.9" data-path="preprocessing.html"><a href="preprocessing.html#normalization-1"><i class="fa fa-check"></i><b>8.9</b> Normalization</a><ul>
<li class="chapter" data-level="8.9.1" data-path="preprocessing.html"><a href="preprocessing.html#min-max-normalization"><i class="fa fa-check"></i><b>8.9.1</b> Min-Max Normalization</a></li>
<li class="chapter" data-level="8.9.2" data-path="preprocessing.html"><a href="preprocessing.html#z-score-normalization"><i class="fa fa-check"></i><b>8.9.2</b> Z-score normalization</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="preprocessing.html"><a href="preprocessing.html#transformations"><i class="fa fa-check"></i><b>8.10</b> Transformations</a><ul>
<li class="chapter" data-level="8.10.1" data-path="preprocessing.html"><a href="preprocessing.html#linear-transformations-and-quadratic-trans-formations"><i class="fa fa-check"></i><b>8.10.1</b> Linear Transformations and Quadratic Trans formations</a></li>
<li class="chapter" data-level="8.10.2" data-path="preprocessing.html"><a href="preprocessing.html#box-cox-transformation"><i class="fa fa-check"></i><b>8.10.2</b> Box-cox transformation</a></li>
<li class="chapter" data-level="8.10.3" data-path="preprocessing.html"><a href="preprocessing.html#nominal-to-binary-tranformations"><i class="fa fa-check"></i><b>8.10.3</b> Nominal to Binary tranformations</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="preprocessing.html"><a href="preprocessing.html#preprocessing-in-r"><i class="fa fa-check"></i><b>8.11</b> Preprocessing in R</a><ul>
<li class="chapter" data-level="8.11.1" data-path="preprocessing.html"><a href="preprocessing.html#the-dplyr-package"><i class="fa fa-check"></i><b>8.11.1</b> The <code>dplyr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.12" data-path="preprocessing.html"><a href="preprocessing.html#other-libraries-and-tricks"><i class="fa fa-check"></i><b>8.12</b> Other libraries and tricks</a></li>
</ul></li>
<li class="part"><span><b>VI Supervised Models</b></span></li>
<li class="chapter" data-level="9" data-path="supervised-classification.html"><a href="supervised-classification.html"><i class="fa fa-check"></i><b>9</b> Supervised Classification</a><ul>
<li class="chapter" data-level="9.1" data-path="supervised-classification.html"><a href="supervised-classification.html#classification-trees"><i class="fa fa-check"></i><b>9.1</b> Classification Trees</a></li>
<li class="chapter" data-level="9.2" data-path="supervised-classification.html"><a href="supervised-classification.html#rules"><i class="fa fa-check"></i><b>9.2</b> Rules</a></li>
<li class="chapter" data-level="9.3" data-path="supervised-classification.html"><a href="supervised-classification.html#distanced-based-methods"><i class="fa fa-check"></i><b>9.3</b> Distanced-based Methods</a></li>
<li class="chapter" data-level="9.4" data-path="supervised-classification.html"><a href="supervised-classification.html#neural-networks"><i class="fa fa-check"></i><b>9.4</b> Neural Networks</a></li>
<li class="chapter" data-level="9.5" data-path="supervised-classification.html"><a href="supervised-classification.html#support-vector-machine"><i class="fa fa-check"></i><b>9.5</b> Support Vector Machine</a></li>
<li class="chapter" data-level="9.6" data-path="supervised-classification.html"><a href="supervised-classification.html#probabilistic-methods"><i class="fa fa-check"></i><b>9.6</b> Probabilistic Methods</a><ul>
<li class="chapter" data-level="9.6.1" data-path="supervised-classification.html"><a href="supervised-classification.html#naive-bayes"><i class="fa fa-check"></i><b>9.6.1</b> Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="supervised-classification.html"><a href="supervised-classification.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.7</b> Linear Discriminant Analysis (LDA)</a><ul>
<li class="chapter" data-level="9.7.1" data-path="supervised-classification.html"><a href="supervised-classification.html#predicting-the-number-of-defects-numerical-class"><i class="fa fa-check"></i><b>9.7.1</b> Predicting the number of defects (numerical class)</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="supervised-classification.html"><a href="supervised-classification.html#binary-logistic-regression-blr"><i class="fa fa-check"></i><b>9.8</b> Binary Logistic Regression (BLR)</a></li>
<li class="chapter" data-level="9.9" data-path="supervised-classification.html"><a href="supervised-classification.html#the-caret-package"><i class="fa fa-check"></i><b>9.9</b> The caret package</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>10</b> Regression</a><ul>
<li class="chapter" data-level="10.1" data-path="regression.html"><a href="regression.html#linear-regression-modeling"><i class="fa fa-check"></i><b>10.1</b> Linear Regression modeling</a><ul>
<li class="chapter" data-level="10.1.1" data-path="regression.html"><a href="regression.html#regression-galton-data"><i class="fa fa-check"></i><b>10.1.1</b> Regression: Galton Data</a></li>
<li class="chapter" data-level="10.1.2" data-path="regression.html"><a href="regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>10.1.2</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="10.1.3" data-path="regression.html"><a href="regression.html#least-squares"><i class="fa fa-check"></i><b>10.1.3</b> Least Squares</a></li>
<li class="chapter" data-level="10.1.4" data-path="regression.html"><a href="regression.html#linear-regression-in-r"><i class="fa fa-check"></i><b>10.1.4</b> Linear regression in R</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="regression.html"><a href="regression.html#linear-regression-diagnostics"><i class="fa fa-check"></i><b>10.2</b> Linear Regression Diagnostics</a><ul>
<li class="chapter" data-level="10.2.1" data-path="regression.html"><a href="regression.html#simulation-example"><i class="fa fa-check"></i><b>10.2.1</b> Simulation example</a></li>
<li class="chapter" data-level="10.2.2" data-path="regression.html"><a href="regression.html#diagnostics-fro-assessing-the-regression-line"><i class="fa fa-check"></i><b>10.2.2</b> Diagnostics fro assessing the regression line</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="regression.html"><a href="regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="10.3.1" data-path="regression.html"><a href="regression.html#partial-least-squares"><i class="fa fa-check"></i><b>10.3.1</b> Partial Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="regression.html"><a href="regression.html#linear-regression-in-software-effort-estimation"><i class="fa fa-check"></i><b>10.4</b> Linear regression in Software Effort estimation</a></li>
<li class="chapter" data-level="10.5" data-path="regression.html"><a href="regression.html#references"><i class="fa fa-check"></i><b>10.5</b> References</a></li>
</ul></li>
<li class="part"><span><b>VII Unsupervised Models</b></span></li>
<li class="chapter" data-level="11" data-path="unsupervised-or-descriptive-modeling.html"><a href="unsupervised-or-descriptive-modeling.html"><i class="fa fa-check"></i><b>11</b> Unsupervised or Descriptive modeling</a><ul>
<li class="chapter" data-level="11.1" data-path="unsupervised-or-descriptive-modeling.html"><a href="unsupervised-or-descriptive-modeling.html#clustering"><i class="fa fa-check"></i><b>11.1</b> Clustering</a><ul>
<li class="chapter" data-level="11.1.1" data-path="unsupervised-or-descriptive-modeling.html"><a href="unsupervised-or-descriptive-modeling.html#k-means"><i class="fa fa-check"></i><b>11.1.1</b> k-Means</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="unsupervised-or-descriptive-modeling.html"><a href="unsupervised-or-descriptive-modeling.html#association-rules"><i class="fa fa-check"></i><b>11.2</b> Association rules</a></li>
</ul></li>
<li class="part"><span><b>VIII Evaluation</b></span></li>
<li class="chapter" data-level="12" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html"><i class="fa fa-check"></i><b>12</b> Evaluation of Models</a><ul>
<li class="chapter" data-level="12.1" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#building-and-validating-a-model"><i class="fa fa-check"></i><b>12.1</b> Building and Validating a Model</a><ul>
<li class="chapter" data-level="12.1.1" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#holdout-approach"><i class="fa fa-check"></i><b>12.1.1</b> Holdout approach</a></li>
<li class="chapter" data-level="12.1.2" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#cross-validation-cv"><i class="fa fa-check"></i><b>12.1.2</b> Cross Validation (CV)</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#evaluation-of-classification-models"><i class="fa fa-check"></i><b>12.2</b> Evaluation of Classification Models</a><ul>
<li class="chapter" data-level="12.2.1" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#prediction-in-probabilistic-classifiers"><i class="fa fa-check"></i><b>12.2.1</b> Prediction in probabilistic classifiers</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#other-metrics-used-in-software-engineering-with-classification"><i class="fa fa-check"></i><b>12.3</b> Other Metrics used in Software Engineering with Classification</a></li>
<li class="chapter" data-level="12.4" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#graphical-evaluation"><i class="fa fa-check"></i><b>12.4</b> Graphical Evaluation</a><ul>
<li class="chapter" data-level="12.4.1" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#receiver-operating-characteristic-roc"><i class="fa fa-check"></i><b>12.4.1</b> Receiver Operating Characteristic (ROC)</a></li>
<li class="chapter" data-level="12.4.2" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#precision-recall-curve-prc"><i class="fa fa-check"></i><b>12.4.2</b> Precision-Recall Curve (PRC)</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#numeric-prediction-evaluation"><i class="fa fa-check"></i><b>12.5</b> Numeric Prediction Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="evaluationSE.html"><a href="evaluationSE.html"><i class="fa fa-check"></i><b>13</b> Measures of Evaluation in Software Engineering</a><ul>
<li class="chapter" data-level="13.1" data-path="evaluationSE.html"><a href="evaluationSE.html#evaluation-of-the-model-in-the-testing-data"><i class="fa fa-check"></i><b>13.1</b> Evaluation of the model in the Testing data</a></li>
<li class="chapter" data-level="13.2" data-path="evaluationSE.html"><a href="evaluationSE.html#building-a-linear-model-on-the-telecom1-dataset"><i class="fa fa-check"></i><b>13.2</b> Building a Linear Model on the Telecom1 dataset</a></li>
<li class="chapter" data-level="13.3" data-path="evaluationSE.html"><a href="evaluationSE.html#building-a-linear-model-on-the-telecom1-dataset-with-all-observations"><i class="fa fa-check"></i><b>13.3</b> Building a Linear Model on the Telecom1 dataset with all observations</a></li>
<li class="chapter" data-level="13.4" data-path="evaluationSE.html"><a href="evaluationSE.html#standardised-accuracy.-marp0.-chinatest"><i class="fa fa-check"></i><b>13.4</b> Standardised Accuracy. MARP0. ChinaTest</a></li>
<li class="chapter" data-level="13.5" data-path="evaluationSE.html"><a href="evaluationSE.html#standardised-accuracy.-marp0.-telecom1"><i class="fa fa-check"></i><b>13.5</b> Standardised Accuracy. MARP0. Telecom1</a><ul>
<li class="chapter" data-level="13.5.1" data-path="evaluationSE.html"><a href="evaluationSE.html#marp0-in-the-atkinson-dataset"><i class="fa fa-check"></i><b>13.5.1</b> MARP0 in the Atkinson dataset</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="evaluationSE.html"><a href="evaluationSE.html#exact-marp0"><i class="fa fa-check"></i><b>13.6</b> Exact MARP0</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html"><a href="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html"><i class="fa fa-check"></i><b>14</b> WBL simple R code to calculate Shepperd and MacDonell’s marp0 exactly</a><ul>
<li class="chapter" data-level="14.1" data-path="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html"><a href="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html#computing-the-bootstraped-confidence-interval-of-the-mean-for-the-test-observations-of-the-china-dataset"><i class="fa fa-check"></i><b>14.1</b> Computing the bootstraped confidence interval of the mean for the Test observations of the China dataset:</a></li>
</ul></li>
<li class="part"><span><b>IX Advanced Topics</b></span></li>
<li class="chapter" data-level="15" data-path="feature-selection-1.html"><a href="feature-selection-1.html"><i class="fa fa-check"></i><b>15</b> Feature Selection</a><ul>
<li class="chapter" data-level="15.1" data-path="feature-selection-1.html"><a href="feature-selection-1.html#instance-selection-1"><i class="fa fa-check"></i><b>15.1</b> Instance Selection</a></li>
<li class="chapter" data-level="15.2" data-path="feature-selection-1.html"><a href="feature-selection-1.html#missing-data-imputation"><i class="fa fa-check"></i><b>15.2</b> Missing Data Imputation</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="feature-selection-example.html"><a href="feature-selection-example.html"><i class="fa fa-check"></i><b>16</b> Feature Selection Example</a></li>
<li class="chapter" data-level="17" data-path="advanced-models.html"><a href="advanced-models.html"><i class="fa fa-check"></i><b>17</b> Advanced Models</a><ul>
<li class="chapter" data-level="17.1" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-for-symbolic-regression"><i class="fa fa-check"></i><b>17.1</b> Genetic Programming for Symbolic Regression</a></li>
<li class="chapter" data-level="17.2" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-example"><i class="fa fa-check"></i><b>17.2</b> Genetic Programming Example</a><ul>
<li class="chapter" data-level="17.2.1" data-path="advanced-models.html"><a href="advanced-models.html#load-data"><i class="fa fa-check"></i><b>17.2.1</b> Load Data</a></li>
<li class="chapter" data-level="17.2.2" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-for-symbolic-regression-china-dataset."><i class="fa fa-check"></i><b>17.2.2</b> Genetic Programming for Symbolic Regression: China dataset.</a></li>
<li class="chapter" data-level="17.2.3" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-for-symbolic-regression.-telecom1-dataset."><i class="fa fa-check"></i><b>17.2.3</b> Genetic Programming for Symbolic Regression. Telecom1 dataset.</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="advanced-models.html"><a href="advanced-models.html#neural-networks-1"><i class="fa fa-check"></i><b>17.3</b> Neural Networks</a></li>
<li class="chapter" data-level="17.4" data-path="advanced-models.html"><a href="advanced-models.html#support-vector-machines"><i class="fa fa-check"></i><b>17.4</b> Support Vector Machines</a></li>
<li class="chapter" data-level="17.5" data-path="advanced-models.html"><a href="advanced-models.html#ensembles"><i class="fa fa-check"></i><b>17.5</b> Ensembles</a><ul>
<li class="chapter" data-level="17.5.1" data-path="advanced-models.html"><a href="advanced-models.html#bagging"><i class="fa fa-check"></i><b>17.5.1</b> Bagging</a></li>
<li class="chapter" data-level="17.5.2" data-path="advanced-models.html"><a href="advanced-models.html#boosting"><i class="fa fa-check"></i><b>17.5.2</b> Boosting</a></li>
<li class="chapter" data-level="17.5.3" data-path="advanced-models.html"><a href="advanced-models.html#rotation-forests"><i class="fa fa-check"></i><b>17.5.3</b> Rotation Forests</a></li>
<li class="chapter" data-level="17.5.4" data-path="advanced-models.html"><a href="advanced-models.html#boosting-in-r"><i class="fa fa-check"></i><b>17.5.4</b> Boosting in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="further-classification-models.html"><a href="further-classification-models.html"><i class="fa fa-check"></i><b>18</b> Further Classification Models</a><ul>
<li class="chapter" data-level="18.1" data-path="further-classification-models.html"><a href="further-classification-models.html#multilabel-classification"><i class="fa fa-check"></i><b>18.1</b> Multilabel classification</a></li>
<li class="chapter" data-level="18.2" data-path="further-classification-models.html"><a href="further-classification-models.html#semi-supervised-learning"><i class="fa fa-check"></i><b>18.2</b> Semi-supervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="social-network-analysis-in-se.html"><a href="social-network-analysis-in-se.html"><i class="fa fa-check"></i><b>19</b> Social Network Analysis in SE</a></li>
<li class="chapter" data-level="20" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html"><i class="fa fa-check"></i><b>20</b> Text Mining Software Engineering Data</a><ul>
<li class="chapter" data-level="20.1" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html#terminology"><i class="fa fa-check"></i><b>20.1</b> Terminology</a></li>
<li class="chapter" data-level="20.2" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html#example-of-classifying-bugs-from-bugzilla"><i class="fa fa-check"></i><b>20.2</b> Example of classifying bugs from Bugzilla</a></li>
<li class="chapter" data-level="20.3" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html#extracting-data-from-twitter"><i class="fa fa-check"></i><b>20.3</b> Extracting data from Twitter</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>21</b> Time Series</a><ul>
<li class="chapter" data-level="21.1" data-path="time-series.html"><a href="time-series.html#web-tutorials-about-time-series"><i class="fa fa-check"></i><b>21.1</b> Web tutorials about Time Series:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis in Software Engineering using R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="advanced-models" class="section level1">
<h1><span class="header-section-number">Chapter 17</span> Advanced Models</h1>
<div id="genetic-programming-for-symbolic-regression" class="section level2">
<h2><span class="header-section-number">17.1</span> Genetic Programming for Symbolic Regression</h2>
<p>This technique is inspired by Darwin’s evolution theory.
+ 1960s by I. Rechenberg in his work “Evolution strategies“
+ 1975 Genetic Algorithms (GAs) invented by J Holland and published in his book”Adaption in Natural and Artificial Systems“
+ 1992 J. Koza has used genetic algorithm to evolve programs to perform certain tasks. He called his method “genetic programming”</p>
<p>Other reference for GP: Langdon WB, Poli R (2001) Foundations of Genetic Programming. Springer.</p>
<p><img src="figures/gpEvolution.png" /></p>
<ul>
<li>Depending on the function set used and the function to be minimised, GP can generate almost any type of curve</li>
</ul>
<p><img src="figures/gp1.png" />
<img src="figures/gp2.png" /></p>
<div class="figure">
<img src="figures/evoAlg.png" alt="Evolutionary Algorithms" />
<p class="caption">Evolutionary Algorithms</p>
</div>
<p>In R, we can use the “rgp” package</p>
</div>
<div id="genetic-programming-example" class="section level2">
<h2><span class="header-section-number">17.2</span> Genetic Programming Example</h2>
<div id="load-data" class="section level3">
<h3><span class="header-section-number">17.2.1</span> Load Data</h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(foreign)

<span class="co">#read data</span>
telecom1 &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;./datasets/effortEstimation/Telecom1.csv&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;,&quot;</span>,<span class="dt">header=</span><span class="ot">TRUE</span>, <span class="dt">stringsAsFactors=</span><span class="ot">FALSE</span>, <span class="dt">dec =</span> <span class="st">&quot;.&quot;</span>) 
 
size_telecom1 &lt;-<span class="st"> </span>telecom1<span class="op">$</span>size
effort_telecom1 &lt;-<span class="st"> </span>telecom1<span class="op">$</span>effort

chinaTrain &lt;-<span class="st"> </span><span class="kw">read.arff</span>(<span class="st">&quot;./datasets/effortEstimation/china3AttSelectedAFPTrain.arff&quot;</span>)
china_train_size &lt;-<span class="st"> </span>chinaTrain<span class="op">$</span>AFP 
china_train_effort &lt;-<span class="st"> </span>chinaTrain<span class="op">$</span>Effort
chinaTest &lt;-<span class="st"> </span><span class="kw">read.arff</span>(<span class="st">&quot;./datasets/effortEstimation/china3AttSelectedAFPTest.arff&quot;</span>)
china_size_test &lt;-<span class="st"> </span>chinaTest<span class="op">$</span>AFP
actualEffort &lt;-<span class="st"> </span>chinaTest<span class="op">$</span>Effort</code></pre>
</div>
<div id="genetic-programming-for-symbolic-regression-china-dataset." class="section level3">
<h3><span class="header-section-number">17.2.2</span> Genetic Programming for Symbolic Regression: China dataset.</h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;rgp&quot;</span>)
<span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">5</span>)
stepsGenerations &lt;-<span class="st"> </span><span class="dv">100</span>
initialPopulation &lt;-<span class="st"> </span><span class="dv">100</span>
Steps &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>)
y &lt;-<span class="st"> </span>china_train_effort   <span class="co">#</span>
x &lt;-<span class="st"> </span>china_train_size  <span class="co"># </span>

data2 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(y, x)  <span class="co"># create a data frame with effort, size</span>
<span class="co"># newFuncSet &lt;- mathFunctionSet</span>
<span class="co"># alternatives to mathFunctionSet</span>
<span class="co"># newFuncSet &lt;- expLogFunctionSet # sqrt&quot;, &quot;exp&quot;, and &quot;ln&quot;</span>
<span class="co"># newFuncSet &lt;- trigonometricFunctionSet</span>
<span class="co"># newFuncSet &lt;- arithmeticFunctionSet</span>
newFuncSet &lt;-<span class="st"> </span><span class="kw">functionSet</span>(<span class="st">&quot;+&quot;</span>,<span class="st">&quot;-&quot;</span>,<span class="st">&quot;*&quot;</span>, <span class="st">&quot;/&quot;</span>,<span class="st">&quot;sqrt&quot;</span>, <span class="st">&quot;log&quot;</span>, <span class="st">&quot;exp&quot;</span>) <span class="co"># ,, )</span>

gpresult &lt;-<span class="st"> </span><span class="kw">symbolicRegression</span>(y <span class="op">~</span><span class="st"> </span>x, 
                                <span class="dt">data=</span>data2, <span class="dt">functionSet=</span>newFuncSet,
                                <span class="dt">populationSize=</span>initialPopulation,
                                <span class="dt">stopCondition=</span><span class="kw">makeStepsStopCondition</span>(stepsGenerations))

bf &lt;-<span class="st"> </span>gpresult<span class="op">$</span>population[[<span class="kw">which.min</span>(<span class="kw">sapply</span>(gpresult<span class="op">$</span>population, gpresult<span class="op">$</span>fitnessFunction))]]
wf &lt;-<span class="st"> </span>gpresult<span class="op">$</span>population[[<span class="kw">which.max</span>(<span class="kw">sapply</span>(gpresult<span class="op">$</span>population, gpresult<span class="op">$</span>fitnessFunction))]]

bf1 &lt;-<span class="st"> </span>gpresult<span class="op">$</span>population[[<span class="kw">which.min</span>((gpresult<span class="op">$</span>fitnessValues))]]
<span class="kw">plot</span>(x,y)
<span class="kw">lines</span>(x, <span class="kw">bf</span>(x), <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">lines</span>(x,<span class="kw">wf</span>(x), <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre>
<p><img src="DASE_files/figure-html/geneticProgramingExample-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r">x_test &lt;-<span class="st"> </span>china_size_test
estim_by_gp &lt;-<span class="st"> </span><span class="kw">bf</span>(x_test)
ae_gp &lt;-<span class="st"> </span><span class="kw">abs</span>(actualEffort <span class="op">-</span><span class="st"> </span>estim_by_gp)
<span class="kw">mean</span>(ae_gp)</code></pre>
<pre><code>## [1] 2135.4</code></pre>
</div>
<div id="genetic-programming-for-symbolic-regression.-telecom1-dataset." class="section level3">
<h3><span class="header-section-number">17.2.3</span> Genetic Programming for Symbolic Regression. Telecom1 dataset.</h3>
<ul>
<li>For illustration purposes only. We use all data points.</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># y &lt;- effort_telecom1   # all data points</span>
<span class="co"># x &lt;- size_telecom1   # </span>
<span class="co"># </span>
<span class="co"># data2 &lt;- data.frame(y, x)  # create a data frame with effort, size</span>
<span class="co"># # newFuncSet &lt;- mathFunctionSet</span>
<span class="co"># # alternatives to mathFunctionSet</span>
<span class="co"># newFuncSet &lt;- expLogFunctionSet # sqrt&quot;, &quot;exp&quot;, and &quot;ln&quot;</span>
<span class="co"># # newFuncSet &lt;- trigonometricFunctionSet</span>
<span class="co"># # newFuncSet &lt;- arithmeticFunctionSet</span>
<span class="co"># # newFuncSet &lt;- functionSet(&quot;+&quot;,&quot;-&quot;,&quot;*&quot;, &quot;/&quot;,&quot;sqrt&quot;, &quot;log&quot;, &quot;exp&quot;) # ,, )</span>
<span class="co"># </span>
<span class="co"># gpresult &lt;- symbolicRegression(y ~ x, </span>
<span class="co">#                                 data=data2, functionSet=newFuncSet,</span>
<span class="co">#                                 populationSize=initialPopulation,</span>
<span class="co">#                                 stopCondition=makeStepsStopCondition(stepsGenerations))</span>
<span class="co"># </span>
<span class="co"># bf &lt;- gpresult$population[[which.min(sapply(gpresult$population, gpresult$fitnessFunction))]]</span>
<span class="co"># wf &lt;- gpresult$population[[which.max(sapply(gpresult$population, gpresult$fitnessFunction))]]</span>
<span class="co"># </span>
<span class="co"># bf1 &lt;- gpresult$population[[which.min((gpresult$fitnessValues))]]</span>
<span class="co"># plot(x,y)</span>
<span class="co"># lines(x, bf(x), type = &quot;l&quot;, col=&quot;blue&quot;, lwd=3)</span>
<span class="co"># lines(x,wf(x), type = &quot;l&quot;, col=&quot;red&quot;, lwd=2)</span></code></pre>
</div>
</div>
<div id="neural-networks-1" class="section level2">
<h2><span class="header-section-number">17.3</span> Neural Networks</h2>
<p>A neural network (NN) simulates some of the learning functions of the human brain.</p>
<p>It can recognize patterns and “learn” . Through the use of a trial and error method the system “learns” to become an “expert” in the field.</p>
<p>A NN is composed of a set of nodes (units, neurons, processing elements)
+ Each node has input and output
+ Each node performs a simple computation by its node function</p>
<p>Weighted connections between nodes
+ Connectivity gives the structure/architecture of the net
+ What can be computed by a NN is primarily determined by the connections and their weights</p>
<p><img src="figures/neuralnet.png" />
<img src="figures/neuralnet2.png" /></p>
<p>There are several packages in R to work with NNs
+ <a href="https://cran.r-project.org/web/packages/neuralnet/index.html">neuralnet</a>
+ <a href="https://cran.r-project.org/web/packages/nnet/index.html">nnet</a>
+ <a href="https://cran.r-project.org/web/packages/RSNNS/index.html">RSNNS</a></p>
<p>TO BE FIXED!!!: The following is an example with the neuralnet package (TO DO, denormalize!). Neural nets need scaling of variables to work properly.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(foreign)
<span class="kw">library</span>(neuralnet)

chinaTrain &lt;-<span class="st"> </span><span class="kw">read.arff</span>(<span class="st">&quot;datasets/effortEstimation/china3AttSelectedAFPTrain.arff&quot;</span>)

afpsize &lt;-<span class="st"> </span>chinaTrain<span class="op">$</span>AFP
effort_china &lt;-<span class="st"> </span>chinaTrain<span class="op">$</span>Effort

chinaTest &lt;-<span class="st"> </span><span class="kw">read.arff</span>(<span class="st">&quot;datasets/effortEstimation/china3AttSelectedAFPTest.arff&quot;</span>)
AFPTest &lt;-<span class="st"> </span>chinaTest<span class="op">$</span>AFP
actualEffort &lt;-<span class="st"> </span>chinaTest<span class="op">$</span>Effort

trainingdata &lt;-<span class="st"> </span><span class="kw">cbind</span>(afpsize,effort_china)
<span class="kw">colnames</span>(trainingdata) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Input&quot;</span>,<span class="st">&quot;Output&quot;</span>)

testingdata &lt;-<span class="st"> </span><span class="kw">cbind</span>(afpsize,effort_china)
<span class="kw">colnames</span>(trainingdata) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Input&quot;</span>,<span class="st">&quot;Output&quot;</span>)

<span class="co">#Normalize data</span>
norm.fun =<span class="st"> </span><span class="cf">function</span>(x){(x <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(x))<span class="op">/</span>(<span class="kw">max</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(x))}
data.norm =<span class="st"> </span><span class="kw">apply</span>(trainingdata, <span class="dv">2</span>, norm.fun)
<span class="co">#data.norm</span>

testdata.norm &lt;-<span class="st"> </span><span class="kw">apply</span>(trainingdata, <span class="dv">2</span>, norm.fun)
<span class="co">#testdata.norm</span>


<span class="co">#Train the neural network</span>
<span class="co">#Going to have 10 hidden layers</span>
<span class="co">#Threshold is a numeric value specifying the threshold for the partial</span>
<span class="co">#derivatives of the error function as stopping criteria.</span>
<span class="co">#net_eff &lt;- neuralnet(Output~Input,trainingdata, hidden=5, threshold=0.25)</span>
net_eff &lt;-<span class="st"> </span><span class="kw">neuralnet</span>(Output<span class="op">~</span>Input, data.norm, <span class="dt">hidden=</span><span class="dv">10</span>, <span class="dt">threshold=</span><span class="fl">0.01</span>)

<span class="co"># Print the network</span>
<span class="co"># print(net_eff)</span>

<span class="co">#Plot the neural network</span>
<span class="kw">plot</span>(net_eff)

<span class="co">#Test the neural network on some training data</span>
<span class="co">#testdata.norm&lt;-data.frame((testdata[,1] - min(data[, &#39;displ&#39;]))/(max(data[, &#39;displ&#39;])-min(data[, &#39;displ&#39;])),(testdata[,2] - min(data[, &#39;year&#39;]))/(max(data[, &#39;year&#39;])-min(data[, &#39;year&#39;])),(testdata[,3] - min(data[, &#39;cyl&#39;]))/(max(data[, &#39;cyl&#39;])-min(data[, &#39;cyl&#39;])),(testdata[,4] - min(data[, &#39;hwy&#39;]))/(max(data[, &#39;hwy&#39;])-min(data[, &#39;hwy&#39;])))</span>

<span class="co"># Run them through the neural network</span>
<span class="co"># net.results &lt;- compute(net_eff, testdata.norm[,2]) </span>


<span class="co">#net.results &lt;- compute(net_eff, dataTest.norm) # With normalized data</span>

<span class="co">#Lets see what properties net.sqrt has</span>
<span class="co">#ls(net.results)</span>
<span class="co">#Lets see the results</span>
<span class="co">#print(net.results$net.result)</span>

<span class="co">#Lets display a better version of the results</span>
<span class="co">#cleanoutput &lt;- cbind(testdata.norm[,2],actualEffort,</span>
<span class="co">#                     as.data.frame(net.results$net.result))</span>
<span class="co">#colnames(cleanoutput) &lt;- c(&quot;Input&quot;,&quot;Expected Output&quot;,&quot;Neural Net Output&quot;)</span>
<span class="co">#print(cleanoutput)</span></code></pre>
</div>
<div id="support-vector-machines" class="section level2">
<h2><span class="header-section-number">17.4</span> Support Vector Machines</h2>
<p>SVM</p>
</div>
<div id="ensembles" class="section level2">
<h2><span class="header-section-number">17.5</span> Ensembles</h2>
<p>Ensembles or meta-learners combine multiple models to obtain better predictions i.e., this technique consists in combining single classifiers (sometimes are also called weak classifiers).</p>
<p>A problem with ensembles is that their models are difficult to interpret (they behave as blackboxes) in comparison to
decision trees or rules which provide an explanation of their
decision making process.</p>
<p>They are typically classified as Bagging, Boosting and Stacking (Stacked generalization).</p>
<div id="bagging" class="section level3">
<h3><span class="header-section-number">17.5.1</span> Bagging</h3>
<p>Bagging (also known as Bootstrap aggregating) is an ensemble technique in which a base learner is applied to multiple equal size datasets created from the original data using bootstraping. Predictions are based on voting of the individual predictions. An advantage of bagging is that it does not require any modification to the learning algorithm and takes advantage of the instability of the base classifier to create diversity among individual ensembles so that individual members of the ensemble perform well in different regions of the data. Bagging does not perform well with classifiers if their output is robust to perturbation of the data such as
nearest-neighbour (NN) classifiers.</p>
</div>
<div id="boosting" class="section level3">
<h3><span class="header-section-number">17.5.2</span> Boosting</h3>
<p>Boosting techniques generate multiple models that complement each other inducing models that improve regions of the data where previous induced models preformed poorly. This is achieved by increasing the weights of instances wrongly classified, so new learners focus on those instances. Finally, classification is based on a weighted voted among all members of the ensemble.</p>
<p>In particular, AdaBoost.M1 [15] is a popular boosting algorithm for classification. The set of training examples is assigned an equal weight at the beginning and the weight of instances is either increased or
decreased depending on whether the learner classified that instance incorrectly or not. The following iterations focus on those instances with higher weights. AdaBoost.M1 can be applied to any base learner.</p>
</div>
<div id="rotation-forests" class="section level3">
<h3><span class="header-section-number">17.5.3</span> Rotation Forests</h3>
<p>Rotation Forests [40] combine randomly chosen subsets of attributes (random subspaces) and bagging approaches with principal components feature generation to construct an ensemble of decision trees. Principal Component Analysis is used as a feature selection technique combining subsets of
attributes which are used with a bootstrapped subset of the training data by the base classifier.</p>
</div>
<div id="boosting-in-r" class="section level3">
<h3><span class="header-section-number">17.5.4</span> Boosting in R</h3>
<p>In R, there are three packages to deal with Boosting: gmb, ada and the mboost packages. An example of gbm using the caret package.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load libraries</span>
<span class="kw">library</span>(caret)
<span class="kw">library</span>(pROC)

<span class="co">#################################################</span>
<span class="co"># model it</span>
<span class="co">#################################################</span>

<span class="co"># Get names of caret supported models (just a few - head)</span>
<span class="kw">head</span>(<span class="kw">names</span>(<span class="kw">getModelInfo</span>()))</code></pre>
<pre><code>## [1] &quot;ada&quot;         &quot;AdaBag&quot;      &quot;AdaBoost.M1&quot; &quot;adaboost&quot;    &quot;amdai&quot;      
## [6] &quot;ANFIS&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Show model info and find out what type of model it is</span>
<span class="kw">getModelInfo</span>()<span class="op">$</span>gbm<span class="op">$</span>tags</code></pre>
<pre><code>## [1] &quot;Tree-Based Model&quot;           &quot;Boosting&quot;                  
## [3] &quot;Ensemble Model&quot;             &quot;Implicit Feature Selection&quot;
## [5] &quot;Accepts Case Weights&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">getModelInfo</span>()<span class="op">$</span>gbm<span class="op">$</span>type</code></pre>
<pre><code>## [1] &quot;Regression&quot;     &quot;Classification&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(foreign)
<span class="kw">library</span>(caret)
<span class="kw">library</span>(pROC)

kc1 &lt;-<span class="st"> </span><span class="kw">read.arff</span>(<span class="st">&quot;./datasets/defectPred/D1/KC1.arff&quot;</span>)

<span class="co"># Split data into training and test datasets</span>
<span class="co"># </span><span class="al">TODO</span><span class="co">: Improve this with createDataParticion from Caret</span>
<span class="kw">set.seed</span>(<span class="dv">1234</span>)
ind &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">2</span>, <span class="kw">nrow</span>(kc1), <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.7</span>, <span class="fl">0.3</span>))
kc1.train &lt;-<span class="st"> </span>kc1[ind<span class="op">==</span><span class="dv">1</span>, ]
kc1.test &lt;-<span class="st"> </span>kc1[ind<span class="op">==</span><span class="dv">2</span>, ]


<span class="co"># create caret trainControl object to control the number of cross-validations performed</span>
objControl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&#39;cv&#39;</span>, <span class="dt">number=</span><span class="dv">3</span>, <span class="dt">returnResamp=</span><span class="st">&#39;none&#39;</span>, <span class="dt">summaryFunction =</span> twoClassSummary, <span class="dt">classProbs =</span> <span class="ot">TRUE</span>)


<span class="co"># run model</span>
objModel &lt;-<span class="st"> </span><span class="kw">train</span>(Defective <span class="op">~</span><span class="st"> </span>.,
                  <span class="dt">data =</span> kc1.train,
                  <span class="dt">method =</span> <span class="st">&#39;gbm&#39;</span>, 
                  <span class="dt">trControl =</span> objControl,  
                  <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span> <span class="co">#,</span>
                  <span class="co">#preProc = c(&quot;center&quot;, &quot;scale&quot;)</span>
                  )</code></pre>
<pre><code>## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.8424            -nan     0.1000    0.0102
##      2        0.8182            -nan     0.1000    0.0089
##      3        0.7997            -nan     0.1000    0.0067
##      4        0.7873            -nan     0.1000    0.0055
##      5        0.7786            -nan     0.1000    0.0030
##      6        0.7714            -nan     0.1000    0.0020
##      7        0.7640            -nan     0.1000    0.0024
##      8        0.7544            -nan     0.1000    0.0048
##      9        0.7463            -nan     0.1000    0.0032
##     10        0.7410            -nan     0.1000    0.0015
##     20        0.7080            -nan     0.1000    0.0006
##     40        0.6832            -nan     0.1000   -0.0002
##     60        0.6699            -nan     0.1000   -0.0003
##     80        0.6563            -nan     0.1000   -0.0004
##    100        0.6472            -nan     0.1000    0.0001
##    120        0.6391            -nan     0.1000   -0.0003
##    140        0.6308            -nan     0.1000   -0.0010
##    150        0.6260            -nan     0.1000   -0.0003
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.8310            -nan     0.1000    0.0133
##      2        0.8072            -nan     0.1000    0.0089
##      3        0.7864            -nan     0.1000    0.0081
##      4        0.7689            -nan     0.1000    0.0064
##      5        0.7575            -nan     0.1000    0.0047
##      6        0.7422            -nan     0.1000    0.0042
##      7        0.7320            -nan     0.1000    0.0038
##      8        0.7230            -nan     0.1000    0.0017
##      9        0.7120            -nan     0.1000    0.0029
##     10        0.7058            -nan     0.1000    0.0013
##     20        0.6618            -nan     0.1000    0.0015
##     40        0.6252            -nan     0.1000   -0.0012
##     60        0.5999            -nan     0.1000   -0.0010
##     80        0.5761            -nan     0.1000   -0.0004
##    100        0.5596            -nan     0.1000    0.0000
##    120        0.5385            -nan     0.1000   -0.0004
##    140        0.5240            -nan     0.1000   -0.0011
##    150        0.5158            -nan     0.1000   -0.0004
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.8345            -nan     0.1000    0.0118
##      2        0.8100            -nan     0.1000    0.0114
##      3        0.7857            -nan     0.1000    0.0083
##      4        0.7714            -nan     0.1000    0.0058
##      5        0.7544            -nan     0.1000    0.0049
##      6        0.7401            -nan     0.1000    0.0067
##      7        0.7312            -nan     0.1000    0.0022
##      8        0.7198            -nan     0.1000    0.0033
##      9        0.7109            -nan     0.1000    0.0014
##     10        0.7021            -nan     0.1000    0.0028
##     20        0.6524            -nan     0.1000   -0.0004
##     40        0.6004            -nan     0.1000   -0.0004
##     60        0.5621            -nan     0.1000    0.0000
##     80        0.5350            -nan     0.1000   -0.0017
##    100        0.5077            -nan     0.1000   -0.0004
##    120        0.4899            -nan     0.1000   -0.0018
##    140        0.4690            -nan     0.1000   -0.0007
##    150        0.4577            -nan     0.1000   -0.0014
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.8434            -nan     0.1000    0.0111
##      2        0.8213            -nan     0.1000    0.0092
##      3        0.8043            -nan     0.1000    0.0075
##      4        0.7867            -nan     0.1000    0.0050
##      5        0.7735            -nan     0.1000    0.0050
##      6        0.7650            -nan     0.1000    0.0033
##      7        0.7572            -nan     0.1000    0.0022
##      8        0.7500            -nan     0.1000    0.0010
##      9        0.7425            -nan     0.1000    0.0032
##     10        0.7395            -nan     0.1000    0.0007
##     20        0.7068            -nan     0.1000    0.0002
##     40        0.6827            -nan     0.1000   -0.0005
##     60        0.6672            -nan     0.1000   -0.0006
##     80        0.6570            -nan     0.1000   -0.0005
##    100        0.6460            -nan     0.1000    0.0004
##    120        0.6387            -nan     0.1000   -0.0005
##    140        0.6309            -nan     0.1000   -0.0003
##    150        0.6273            -nan     0.1000   -0.0003
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.8252            -nan     0.1000    0.0149
##      2        0.8040            -nan     0.1000    0.0074
##      3        0.7862            -nan     0.1000    0.0052
##      4        0.7709            -nan     0.1000    0.0062
##      5        0.7549            -nan     0.1000    0.0062
##      6        0.7423            -nan     0.1000    0.0033
##      7        0.7322            -nan     0.1000    0.0024
##      8        0.7246            -nan     0.1000    0.0019
##      9        0.7188            -nan     0.1000    0.0006
##     10        0.7103            -nan     0.1000    0.0030
##     20        0.6715            -nan     0.1000    0.0003
##     40        0.6339            -nan     0.1000   -0.0015
##     60        0.6057            -nan     0.1000   -0.0002
##     80        0.5866            -nan     0.1000   -0.0012
##    100        0.5689            -nan     0.1000   -0.0003
##    120        0.5520            -nan     0.1000   -0.0004
##    140        0.5372            -nan     0.1000   -0.0007
##    150        0.5295            -nan     0.1000   -0.0007
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.8347            -nan     0.1000    0.0162
##      2        0.8090            -nan     0.1000    0.0107
##      3        0.7864            -nan     0.1000    0.0085
##      4        0.7725            -nan     0.1000    0.0028
##      5        0.7588            -nan     0.1000    0.0026
##      6        0.7448            -nan     0.1000    0.0041
##      7        0.7348            -nan     0.1000    0.0024
##      8        0.7237            -nan     0.1000    0.0025
##      9        0.7178            -nan     0.1000    0.0012
##     10        0.7049            -nan     0.1000    0.0041
##     20        0.6472            -nan     0.1000   -0.0004
##     40        0.6046            -nan     0.1000   -0.0008
##     60        0.5684            -nan     0.1000   -0.0014
##     80        0.5432            -nan     0.1000   -0.0011
##    100        0.5152            -nan     0.1000   -0.0010
##    120        0.4900            -nan     0.1000   -0.0009
##    140        0.4707            -nan     0.1000   -0.0008
##    150        0.4625            -nan     0.1000   -0.0013
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.8337            -nan     0.1000    0.0127
##      2        0.8137            -nan     0.1000    0.0085
##      3        0.7882            -nan     0.1000    0.0102
##      4        0.7683            -nan     0.1000    0.0089
##      5        0.7520            -nan     0.1000    0.0071
##      6        0.7408            -nan     0.1000    0.0030
##      7        0.7295            -nan     0.1000    0.0052
##      8        0.7242            -nan     0.1000    0.0006
##      9        0.7140            -nan     0.1000    0.0037
##     10        0.7086            -nan     0.1000    0.0016
##     20        0.6677            -nan     0.1000   -0.0001
##     40        0.6394            -nan     0.1000   -0.0009
##     60        0.6252            -nan     0.1000   -0.0002
##     80        0.6171            -nan     0.1000   -0.0003
##    100        0.6073            -nan     0.1000   -0.0003
##    120        0.6011            -nan     0.1000   -0.0002
##    140        0.5909            -nan     0.1000   -0.0007
##    150        0.5874            -nan     0.1000   -0.0006
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.8310            -nan     0.1000    0.0169
##      2        0.8017            -nan     0.1000    0.0147
##      3        0.7752            -nan     0.1000    0.0120
##      4        0.7533            -nan     0.1000    0.0070
##      5        0.7366            -nan     0.1000    0.0088
##      6        0.7254            -nan     0.1000    0.0029
##      7        0.7137            -nan     0.1000    0.0047
##      8        0.7040            -nan     0.1000    0.0045
##      9        0.6986            -nan     0.1000   -0.0004
##     10        0.6945            -nan     0.1000    0.0005
##     20        0.6445            -nan     0.1000   -0.0006
##     40        0.5992            -nan     0.1000   -0.0003
##     60        0.5740            -nan     0.1000   -0.0007
##     80        0.5518            -nan     0.1000   -0.0002
##    100        0.5276            -nan     0.1000   -0.0005
##    120        0.5105            -nan     0.1000   -0.0009
##    140        0.4939            -nan     0.1000   -0.0003
##    150        0.4839            -nan     0.1000   -0.0004
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.8219            -nan     0.1000    0.0177
##      2        0.7959            -nan     0.1000    0.0110
##      3        0.7663            -nan     0.1000    0.0124
##      4        0.7469            -nan     0.1000    0.0052
##      5        0.7293            -nan     0.1000    0.0076
##      6        0.7138            -nan     0.1000    0.0062
##      7        0.7004            -nan     0.1000    0.0052
##      8        0.6883            -nan     0.1000    0.0043
##      9        0.6803            -nan     0.1000    0.0023
##     10        0.6712            -nan     0.1000    0.0005
##     20        0.6192            -nan     0.1000   -0.0007
##     40        0.5594            -nan     0.1000   -0.0000
##     60        0.5264            -nan     0.1000   -0.0015
##     80        0.4934            -nan     0.1000   -0.0003
##    100        0.4707            -nan     0.1000   -0.0009
##    120        0.4484            -nan     0.1000   -0.0007
##    140        0.4306            -nan     0.1000   -0.0006
##    150        0.4218            -nan     0.1000   -0.0004
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.8323            -nan     0.1000    0.0146
##      2        0.8053            -nan     0.1000    0.0105
##      3        0.7798            -nan     0.1000    0.0119
##      4        0.7633            -nan     0.1000    0.0072
##      5        0.7492            -nan     0.1000    0.0080
##      6        0.7378            -nan     0.1000    0.0033
##      7        0.7258            -nan     0.1000    0.0053
##      8        0.7136            -nan     0.1000    0.0048
##      9        0.7027            -nan     0.1000    0.0040
##     10        0.6940            -nan     0.1000    0.0022
##     20        0.6475            -nan     0.1000   -0.0009
##     40        0.6085            -nan     0.1000    0.0001
##     60        0.5769            -nan     0.1000   -0.0008
##     80        0.5556            -nan     0.1000   -0.0010
##    100        0.5358            -nan     0.1000    0.0001
##    120        0.5147            -nan     0.1000   -0.0010
##    140        0.4989            -nan     0.1000   -0.0005
##    150        0.4924            -nan     0.1000   -0.0006</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Find out variable importance</span>
<span class="kw">summary</span>(objModel)</code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-99-1.png" width="672" /></p>
<pre><code>##                                         var       rel.inf
## HALSTEAD_CONTENT           HALSTEAD_CONTENT 11.9719209365
## HALSTEAD_DIFFICULTY     HALSTEAD_DIFFICULTY 11.6281222487
## HALSTEAD_EFFORT             HALSTEAD_EFFORT  7.3982708323
## LOC_EXECUTABLE               LOC_EXECUTABLE  6.6547461836
## HALSTEAD_LENGTH             HALSTEAD_LENGTH  6.6481194643
## HALSTEAD_VOLUME             HALSTEAD_VOLUME  6.2651829677
## NUM_OPERANDS                   NUM_OPERANDS  5.9550331042
## LOC_TOTAL                         LOC_TOTAL  5.9217452855
## NUM_UNIQUE_OPERATORS   NUM_UNIQUE_OPERATORS  5.6928518350
## LOC_COMMENTS                   LOC_COMMENTS  5.0355938149
## NUM_UNIQUE_OPERANDS     NUM_UNIQUE_OPERANDS  4.8712914173
## NUM_OPERATORS                 NUM_OPERATORS  4.2714353987
## LOC_BLANK                         LOC_BLANK  4.1242127346
## LOC_CODE_AND_COMMENT   LOC_CODE_AND_COMMENT  2.8625349112
## ESSENTIAL_COMPLEXITY   ESSENTIAL_COMPLEXITY  2.8170600164
## DESIGN_COMPLEXITY         DESIGN_COMPLEXITY  2.5833211480
## CYCLOMATIC_COMPLEXITY CYCLOMATIC_COMPLEXITY  2.3053896008
## BRANCH_COUNT                   BRANCH_COUNT  2.2287410148
## HALSTEAD_ERROR_EST       HALSTEAD_ERROR_EST  0.5509628978
## HALSTEAD_LEVEL               HALSTEAD_LEVEL  0.2134641879
## HALSTEAD_PROG_TIME       HALSTEAD_PROG_TIME  0.0000000000</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># find out model details</span>
objModel</code></pre>
<pre><code>## Stochastic Gradient Boosting 
## 
## 1500 samples
##   21 predictors
##    2 classes: &#39;N&#39;, &#39;Y&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (3 fold) 
## Summary of sample sizes: 1000, 1000, 1000 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  ROC           Sens          Spec        
##   1                   50      0.8064922834  0.9826224329  0.1495726496
##   1                  100      0.8098239964  0.9802527646  0.1923076923
##   1                  150      0.8110442743  0.9794628752  0.2051282051
##   2                   50      0.8137379998  0.9731437599  0.1965811966
##   2                  100      0.8153785393  0.9644549763  0.2393162393
##   2                  150      0.8135405274  0.9644549763  0.2606837607
##   3                   50      0.8161532385  0.9715639810  0.2136752137
##   3                  100      0.8155608215  0.9644549763  0.2905982906
##   3                  150      0.8164165350  0.9668246445  0.3034188034
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were n.trees = 150,
##  interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#################################################</span>
<span class="co"># evalutate model</span>
<span class="co">#################################################</span>
<span class="co"># get predictions on your testing data</span>

<span class="co"># class prediction</span>
predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object=</span>objModel, kc1.test[,<span class="op">-</span><span class="dv">22</span>], <span class="dt">type=</span><span class="st">&#39;raw&#39;</span>)
<span class="kw">head</span>(predictions)</code></pre>
<pre><code>## [1] N N N N N N
## Levels: N Y</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">postResample</span>(<span class="dt">pred=</span>predictions, <span class="dt">obs=</span><span class="kw">as.factor</span>(kc1.test[,<span class="dv">22</span>]))</code></pre>
<pre><code>##     Accuracy        Kappa 
## 0.8657718121 0.2891009393</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># probabilities </span>
predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object=</span>objModel, kc1.test[,<span class="op">-</span><span class="dv">22</span>], <span class="dt">type=</span><span class="st">&#39;prob&#39;</span>)
<span class="kw">head</span>(predictions)</code></pre>
<pre><code>##              N             Y
## 1 0.9185156057 0.08148439428
## 2 0.9797772040 0.02022279595
## 3 0.8470294127 0.15297058732
## 4 0.8799954090 0.12000459096
## 5 0.8298472743 0.17015272568
## 6 0.9578031188 0.04219688123</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">postResample</span>(<span class="dt">pred=</span>predictions[[<span class="dv">2</span>]], <span class="dt">obs=</span><span class="kw">ifelse</span>(kc1.test[,<span class="dv">22</span>]<span class="op">==</span><span class="st">&#39;yes&#39;</span>,<span class="dv">1</span>,<span class="dv">0</span>))</code></pre>
<pre><code>##         RMSE     Rsquared          MAE 
## 0.2165564399           NA 0.1347441023</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">auc &lt;-<span class="st"> </span><span class="kw">roc</span>(<span class="kw">ifelse</span>(kc1.test[,<span class="dv">22</span>]<span class="op">==</span><span class="st">&quot;Y&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>), predictions[[<span class="dv">2</span>]])
<span class="kw">print</span>(auc<span class="op">$</span>auc)</code></pre>
<pre><code>## Area under the curve: 0.8031553</code></pre>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="feature-selection-example.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="further-classification-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/danrodgar/dasedown/edit/master/510_advancedModelBuilding.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["DASE.pdf", "DASE.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
