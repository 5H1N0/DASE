% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@Article{Abran_TSE96_FP,
  Title                    = {Function points analysis: an empirical study of its measurement processes},
  Author                   = {Abran, A. and Robillard, P.N.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {1996},

  Month                    = {dec},
  Number                   = {12},
  Pages                    = {895 -910},
  Volume                   = {22},

  Abstract                 = {Function point analysis (FPA) was initially designed on the basis of expert judgments, without explicit reference to any theoretical foundation. From the point of view of the measurement scales used in its measurement process, FPA constitutes a potpourri of scales not admissible without the transformations imbedded in the implicit models of expert judgments. The results of this empirical study demonstrate that in a homogeneous environment not burdened with major differences in productivity factors there is a clear relationship between FPA's primary components and work-effort. This empirical study also indicates that there is such a relationship for each step of the FPA measurement process prior to the mixing of scales and the assignments of weights. Comparisons with FPA productivity models based on weights confirm, on the one hand, that the weights do not add information and, on the other, that the weights are fairly robust and can be used when little historical data is available. The full data set is provided for future studies},
  Doi                      = {10.1109/32.553638},
  ISSN                     = {0098-5589},
  Keywords                 = {function points analysis;homogeneous environment;measurement processes;measurement scales;productivity factors;weight assignments;work-effort;human resource management;software cost estimation;software metrics;}
}

@Article{Aguilar-RuizRRT01,
  Title                    = {An Evolutionary Approach to Estimating Software Development Projects},
  Author                   = {Jes\'es S. Aguilar-Ruiz and Isabel Ramos and Jos\'e C. Riquelme and Miguel Toro},
  Journal                  = {Information and Software Technology},
  Year                     = {2001},

  Month                    = {December},
  Number                   = {14},
  Pages                    = {875--882},
  Volume                   = {43},

  Doi                      = {http://dx.doi.org/10.1016/S0950-5849(01)00193-8},
  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@InProceedings{Aguilar-RuizRR02,
  Title                    = {Natural Evolutionary Coding: An Application to Estimating Software Development Projects},
  Author                   = {Jes\'us S. Aguilar-Ruiz and Jos\'e Crist\'obal Riquelme and Isabel Ramos},
  Booktitle                = {Proceedings of the 2002 Conference on Genetic and Evolutionary Computation (GECCO '02)},
  Year                     = {2002},

  Address                  = {New York, USA},
  Month                    = {9-13 July},
  Pages                    = {1--8},
  Publisher                = {AAAI},

  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@Article{AlbrechtG83,
  Title                    = {Software Function, Source Lines of Code, and Development Effort Prediction: A Software Science Validation},
  Author                   = {Albrecht, A.J. and Gaffney, J.E., Jr.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {1983},

  Month                    = {nov.},
  Number                   = {6},
  Pages                    = {639--648},
  Volume                   = {9},

  Abstract                 = {One of the most important problems faced by software developers and users is the prediction of the size of a programming system and its development effort. As an alternative to "size," one might deal with a measure of the "function" that the software is to perform. Albrecht [1] has developed a methodology to estimate the amount of the "function" the software is to perform, in terms of the data it is to use (absorb) and to generate (produce). The "function" is quantified as "function points," essentially, a weighted sum of the numbers of "inputs," "outputs,"master files," and "inquiries" provided to, or generated by, the software. This paper demonstrates the equivalence between Albrecht's external input/output data flow representative of a program (the "function points" metric) and Halstead's [2] "software science" or "software linguistics" model of a program as well as the "soft content" variation of Halstead's model suggested by Gaffney [7].},
  Doi                      = {10.1109/TSE.1983.235271},
  ISSN                     = {0098-5589},
  Keywords                 = {Cost estimating; function points; software linguistics;}
}

@Article{AliBHP10,
  Title                    = {A Systematic Review of the Application and Empirical Investigation of Search-Based Test-Case Generation},
  Author                   = {Shaukat Ali and Lionel C. Briand and Hadi Hemmati and Rajwinder Kaur Panesar-Walawege},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2010},

  Month                    = {November-December},
  Number                   = {6},
  Pages                    = {742-762},
  Volume                   = {36},

  Address                  = {Los Alamitos, CA, USA},
  Doi                      = {http://dx.doi.org/10.1109/TSE.2009.52},
  Owner                    = {drg},
  Publisher                = {IEEE Computer Society},
  Timestamp                = {2012.09.15}
}

@Article{Arisholm10,
  Title                    = {A systematic and comprehensive investigation of methods to build and evaluate fault prediction models},
  Author                   = {Erik Arisholm and Lionel C. Briand and Eivind B. Johannessen},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {2--17},
  Volume                   = {83},

  Address                  = {New York, NY, USA},
  Doi                      = {http://dx.doi.org/10.1016/j.jss.2009.06.055},
  ISSN                     = {0164-1212},
  Publisher                = {Elsevier Science Inc.}
}

@Article{BagnallRSW01,
  Title                    = {The Next Release Problem},
  Author                   = {A. J. Bagnall and V. J. Rayward-Smith and I. M. Whittley},
  Journal                  = {Information and Software Technology},
  Year                     = {2001},

  Month                    = {December},
  Number                   = {14},
  Pages                    = {883-890},
  Volume                   = {43},

  Doi                      = {http://dx.doi.org/10.1016/S0950-5849(01)00194-X},
  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@InProceedings{Bailey81,
  Title                    = {A meta-model for software development resource expenditures},
  Author                   = {Bailey, John W. and Basili, Victor R.},
  Booktitle                = {Proceedings of the 5th international conference on Software engineering (ICSE'81)},
  Year                     = {1981},

  Address                  = {Piscataway, NJ, USA},
  Pages                    = {107--116},
  Publisher                = {IEEE Press},
  Series                   = {ICSE'81},

  Acmid                    = {802522},
  ISBN                     = {0-89791-146-6},
  Location                 = {San Diego, California, United States},
  Numpages                 = {10},
  Url                      = {http://dl.acm.org/citation.cfm?id=800078.802522}
}

@Article{Banker1994,
  Title                    = {Evidence on economies of scale in software development},
  Author                   = {Rajiv D Banker and Hsihui Chang and Chris F Kemerer},
  Journal                  = {Information and Software Technology},
  Year                     = {1994},
  Number                   = {5},
  Pages                    = {275 - 282},
  Volume                   = {36},

  Abstract                 = {Researchers and practitioners have found it useful for cost estimation and productivity evaluation purposes to think of software development as an economic production process, whereby inputs, most notably the effort of systems development professionals, are converted into outputs (systems deliverables), often measured as the size of the delivered system. One central issue in developing such models is how to describe the production relationship between the inputs and outputs. In particular, there has been much discussion about the existence of either increasing or decreasing returns to scale. The presence or absence of scale economies at a given size are important to commercial practice in that they influence productivity. A project manager can use this knowledge to scale future projects so as to maximize the productivity of software development effort. The question of whether the software development production process should be modelled with a non-linear model is the subject of some recent controversy. This paper examines the issue of non-linearities through the analysis of 11 datasets using, in addition to standard parametric tests, new statistical tests with the non-parametric Data Envelopment Analysis (DEA) methodology. Results of this analysis support the hypothesis of significant non-linearities, and the existence of both economies and diseconomies of scale in software development.},
  Doi                      = {10.1016/0950-5849(94)90083-3},
  ISSN                     = {0950-5849},
  Keywords                 = {scale economies},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0950584994900833}
}

@InBook{Belady79,
  Title                    = {Research Directions in Software Technology},
  Author                   = {L.A. Belady and M.M. Lehman},
  Chapter                  = {The characteristics of large systems},
  Publisher                = {MIT Press},
  Year                     = {1979},

  Address                  = {Cambridge, MA}
}

@Book{Boehm81,
  Title                    = {Software Engineering Economics},
  Author                   = {Boehm, Barry W.},
  Publisher                = {Prentice Hall PTR},
  Year                     = {1981},

  Address                  = {Upper Saddle River, NJ, USA},
  Edition                  = {1st},

  ISBN                     = {0138221227}
}

@Misc{BMO07,
  Title                    = {PROMISE Repository of empirical software engineering data},

  Author                   = {G. Boetticher and T. Menzies and T. Ostrand},
  Year                     = {2007},

  Organization             = {West Virginia University, Department of Computer Science},
  Url                      = {http://promisedata.org/ repository}
}

@InProceedings{buse-icse-2012,
  Title                    = {Information Needs for Software Development Analytics},
  Author                   = {Raymond P.L. Buse and Thomas Zimmermann},
  Booktitle                = {Proceedings of the 34th International Conference on Software Engineering},
  Year                     = {2012},
  Month                    = {June},

  Location                 = {Zurich, Switzerland}
}

@InProceedings{Caglayan09,
  Title                    = {Merits of using repository metrics in defect prediction for open source projects},
  Author                   = {Caglayan, Bora and Bener, Ayse and Koch, Stefan},
  Booktitle                = {Proceedings of the 2009 ICSE Workshop on Emerging Trends in Free/Libre/Open Source Software Research and Development},
  Year                     = {2009},

  Address                  = {Washington, DC, USA},
  Pages                    = {31--36},
  Publisher                = {IEEE Computer Society},
  Series                   = {FLOSS'09},

  Acmid                    = {1572199},
  Doi                      = {10.1109/FLOSS.2009.5071357},
  ISBN                     = {978-1-4244-3720-7},
  Numpages                 = {6},
  Url                      = {http://dx.doi.org/10.1109/FLOSS.2009.5071357}
}

@Article{CC2009,
  Title                    = {A systematic review of software fault prediction studies},
  Author                   = {Cagatay Catal and Banu Diri},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {7346 - 7354},
  Volume                   = {36},

  Abstract                 = {This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle.},
  Doi                      = {10.1016/j.eswa.2008.10.027},
  ISSN                     = {0957-4174},
  Keywords                 = {Machine learning},
  Owner                    = {drg},
  Timestamp                = {2012.09.15},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417408007215}
}

@InCollection{Chen07DMTools,
  Title                    = {A Survey of Open Source Data Mining Systems},
  Author                   = {Chen, Xiaojun and Ye, Yunming and Williams, Graham and Xu, Xiaofei},
  Booktitle                = {Emerging Technologies in Knowledge Discovery and Data Mining},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2007},
  Editor                   = {Washio, Takashi and Zhou, Zhi-Hua and Huang, JoshuaZhexue and Hu, Xiaohua and Li, Jinyan and Xie, Chao and He, Jieyue and Zou, Deqing and Li, Kuan-Ching and Freire, MÃ¡rioM.},
  Pages                    = {3-14},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {4819},

  ISBN                     = {978-3-540-77016-9}
}

@Article{CHPB05,
  Title                    = {Finding the right data for software cost modeling},
  Author                   = {Chen, Z. and Menzies, T. and Port, D. and Boehm, D.},
  Journal                  = {IEEE Software},
  Year                     = {2005},

  Month                    = {Nov-Dec},
  Number                   = {6},
  Pages                    = {38--46},
  Volume                   = {22},

  Doi                      = {10.1109/MS.2005.151},
  ISSN                     = {0740-7459},
  Keywords                 = { data collection; data-pruning method; real-world data set; resource management; software cost model; software engineering; software project management; project management; software cost estimation; software management;}
}

@Misc{clauset07,
  Title                    = {Power-law distributions in empirical data},

  Author                   = {Aaron Clauset and Cosma Rohilla Shalizi and M.~E.~J. Newman},
  Year                     = {2007},

  Url                      = {http://www.citebase.org/abstract?id=oai:arXiv.org:0706.1062}
}

@Article{5928349,
  Title                    = {On the Distribution of Bugs in the Eclipse System},
  Author                   = {Concas, G. and Marchesi, M. and Murgia, A. and Tonelli, R. and Turnu, I.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2011},

  Month                    = {Nov-Dec},
  Number                   = {6},
  Pages                    = {872--877},
  Volume                   = {37},

  Doi                      = {10.1109/TSE.2011.54},
  ISSN                     = {0098-5589},
  Keywords                 = {Pareto principle;Weibull cumulative distribution;eclipse system;software systems;statistical perspective;Pareto analysis;Weibull distribution;eclipses;}
}

@Article{CWHW12,
  Title                    = {Free/Libre open-source software development: What we know and what we do not know},
  Author                   = {Crowston, Kevin and Wei, Kangning and Howison, James and Wiggins, Andrea},
  Journal                  = {ACM Computing Surveys},
  Year                     = {2008},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {7:1--7:35},
  Volume                   = {44},

  Acmid                    = {2089127},
  Address                  = {New York, NY, USA},
  Articleno                = {7},
  Doi                      = {10.1145/2089125.2089127},
  ISSN                     = {0360-0300},
  Issue_date               = {February 2012},
  Keywords                 = {Free/Libre open-source software, computer-mediated communication, development, distributed work},
  Numpages                 = {35},
  Publisher                = {ACM}
}

@Article{Dean08,
  Title                    = {MapReduce: simplified data processing on large clusters},
  Author                   = {Dean, Jeffrey and Ghemawat, Sanjay},
  Journal                  = {Commun. ACM},
  Year                     = {2008},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {107--113},
  Volume                   = {51},

  Acmid                    = {1327492},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/1327452.1327492},
  ISSN                     = {0001-0782},
  Issue_date               = {January 2008},
  Numpages                 = {7},
  Publisher                = {ACM},
  Url                      = {http://doi.acm.org/10.1145/1327452.1327492}
}

@Article{Dejaeger_TSE12_EffEst,
  Title                    = {Data Mining Techniques for Software Effort Estimation: A Comparative Study},
  Author                   = {Dejaeger, K. and Verbeke, W. and Martens, D. and Baesens, B.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {march-april },
  Number                   = {2},
  Pages                    = {375 -397},
  Volume                   = {38},

  Abstract                 = {A predictive model is required to be accurate and comprehensible in order to inspire confidence in a business setting. Both aspects have been assessed in a software effort estimation setting by previous studies. However, no univocal conclusion as to which technique is the most suited has been reached. This study addresses this issue by reporting on the results of a large scale benchmarking study. Different types of techniques are under consideration, including techniques inducing tree/rule-based models like M5 and CART, linear models such as various types of linear regression, nonlinear models (MARS, multilayered perceptron neural networks, radial basis function networks, and least squares support vector machines), and estimation techniques that do not explicitly induce a model (e.g., a case-based reasoning approach). Furthermore, the aspect of feature subset selection by using a generic backward input selection wrapper is investigated. The results are subjected to rigorous statistical testing and indicate that ordinary least squares regression in combination with a logarithmic transformation performs best. Another key finding is that by selecting a subset of highly predictive attributes such as project size, development, and environment related attributes, typically a significant increase in estimation accuracy can be obtained.},
  Doi                      = {10.1109/TSE.2011.55},
  ISSN                     = {0098-5589},
  Keywords                 = {CART;M5;data mining techniques;estimation techniques;feature subset selection;generic backward input selection wrapper;linear regression;logarithmic transformation;nonlinear models;ordinary least squares regression;predictive model;rigorous statistical testing;rule-based models;software effort estimation;data mining;program testing;regression analysis;software cost estimation;}
}

@Article{Demsar2006,
  Title                    = {Statistical Comparisons of Classifiers over Multiple Data Sets},
  Author                   = {Dem\v{s}ar, Janez},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2006},

  Month                    = dec,
  Pages                    = {1--30},
  Volume                   = {7},

  Acmid                    = {1248548},
  ISSN                     = {1532-4435},
  Issue_date               = {12/1/2006},
  Numpages                 = {30},
  Publisher                = {JMLR.org},
  Url                      = {http://dl.acm.org/citation.cfm?id=1248547.1248548}
}

@PhdThesis{Desharnais88,
  Title                    = {Analyse statistique de la productivite des projects de development en informatique a partir de la technique des points de fonction},
  Author                   = {J.M. Desharnais},
  School                   = {Univ. du Quebec a Montreal},
  Year                     = {1988},
  Type                     = {MSc Thesis}
}

@Article{DER05,
  Title                    = {Supporting Controlled Experimentation with Testing Techniques: An Infrastructure and its Potential Impact},
  Author                   = {Do, Hyunsook and Elbaum, Sebastian and Rothermel, Gregg},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2005},

  Month                    = {October},
  Pages                    = {405--435},
  Volume                   = {10},

  Acmid                    = {1089928},
  Address                  = {Hingham, MA, USA},
  Doi                      = {10.1007/s10664-005-3861-2},
  ISSN                     = {1382-3256},
  Issue                    = {4},
  Keywords                 = {Software testing, controlled experimentation, experiment infrastructure, regression testing},
  Numpages                 = {31},
  Publisher                = {Kluwer Academic Publishers},
  Url                      = {http://dl.acm.org/citation.cfm?id=1089922.1089928}
}

@Article{Dolado01_CostEst,
  Title                    = {On the problem of the software cost function},
  Author                   = {J.J Dolado},
  Journal                  = {Information and Software Technology},
  Year                     = {2001},
  Number                   = {1},
  Pages                    = {61--72},
  Volume                   = {43},

  Abstract                 = {The question of finding a function for software cost estimation is a long-standing issue in the software engineering field. The results of other works have shown different patterns for the unknown function, which relates software size to project cost (effort). In this work, the research about this problem has been made by using the technique of Genetic Programming (GP) for exploring the possible cost functions. Both standard regression analysis and GP have been applied and compared on several data sets. However, regardless of the method, the basic size-effort relationship does not show satisfactory results, from the predictive point of view, across all data sets. One of the results of this work is that we have not found significant deviations from the linear model in the software cost functions. This result comes from the marginal cost analysis of the equations with best predictive values.},
  Doi                      = {10.1016/S0950-5849(00)00137-3},
  ISSN                     = {0950-5849},
  Keywords                 = {Software cost function},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950584900001373}
}

@Article{Dolado97,
  Title                    = {A study of the relationships among Albrecht and Mark II Function Points, lines of code 4GL and effort},
  Author                   = {J.J. Dolado},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1997},
  Number                   = {2},
  Pages                    = {161--173},
  Volume                   = {37},

  Abstract                 = {There is a strong interest in finding metrics for replacing the common LOC measure of software size, with most of the interest focusing on the Function Point measures. Mark II Function Points were proposed as a better technique than the original of Albrecht Function Points. In this work, the results of a study comparing those measures are stated, and they are also compared against effort and LOC. Since other published results are based on samples generated randomly, it is interesting to see both methods working when applied to the same projects. The data collected comes from the measurements of academic projects. The fact that all projects have been developed in the same environment (mostly 4GL) and domain (accounting information systems) allows the value of the technical complexity adjustment variable to be set to constant and also allows us to examine the relationships among the variables. Several conclusions are reported.},
  Doi                      = {10.1016/S0164-1212(96)00111-2},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121296001112}
}

@Misc{EBSE,
  Title                    = {Template for a Systematic Literature Review Protocol},

  Author                   = {EBSE},
  Year                     = {2010},

  Url                      = {http://www.dur.ac.uk/ebse/resources/templates/SLRTemplate.pdf}
}

@Article{FPS96,
  Title                    = {The KDD process for extracting useful knowledge from volumes of data},
  Author                   = {Fayyad, Usama and Piatetsky-Shapiro, Gregory and Smyth, Padhraic},
  Journal                  = {Communications of the ACM},
  Year                     = {1996},

  Month                    = {November},
  Pages                    = {27--34},
  Volume                   = {39},

  Acmid                    = {240464},
  Address                  = {New York, NY, USA},
  Doi                      = {http://doi.acm.org/10.1145/240455.240464},
  ISSN                     = {0001-0782},
  Issue                    = {11},
  Numpages                 = {8},
  Publisher                = {ACM}
}

@InProceedings{FGH11,
  Title                    = {Addressing the Classification with Imbalanced Data: Open Problems and New Challenges on Class Distribution},
  Author                   = {Alberto Fern\'andez and Salvador Garc\'ia and Francisco Herrera},
  Booktitle                = {6th International Conference on Hybrid Artificial Intelligence Systems (HAIS)},
  Year                     = {2011},
  Pages                    = {1--10},

  Ee                       = {http://dx.doi.org/10.1007/978-3-642-21219-2_1}
}

@Article{FinkelsteinHMRZ09,
  Title                    = {A Search based Approach to Fairness Analysis in Requirement Assignments to Aid Negotiation, Mediation and Decision Making},
  Author                   = {Anthony Finkelstein and Mark Harman and S. Afshin Mansouri and Jian Ren and Yuanyuan Zhang},
  Journal                  = {Requirements Engineering Journal (RE'08 Special Issue)},
  Year                     = {2009},

  Month                    = {December},
  Number                   = {4},
  Pages                    = {231--245},
  Volume                   = {14},

  Doi                      = {http://dx.doi.org/10.1007/s00766-009-0075-y},
  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@InProceedings{FlachHR11,
  Title                    = {A Coherent Interpretation of AUC as a Measure of Aggregated Classification Performance},
  Author                   = {Peter A. Flach and Jos{\'e} Hern{\'a}ndez-Orallo and C{\`e}sar Ferri Ramirez},
  Booktitle                = {Proceedings of the 28th International Conference on Machine Learning (ICML'11)},
  Year                     = {2011},

  Address                  = {Bellevue, Washington, USA},
  Month                    = {June 28- July 2, 2011},
  Pages                    = {657--664},

  Bibsource                = {DBLP, http://dblp.uni-trier.de}
}

@Article{GR12,
  Title                    = {On the reproducibility of empirical software engineering studies based on data retrieved from development repositories},
  Author                   = {Gonz\'alez-Barahona, Jes\'us and Robles, Gregorio},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2012},
  Note                     = {10.1007/s10664-011-9181-9},
  Pages                    = {75-89},
  Volume                   = {17},

  Affiliation              = {Universidad Rey Juan Carlos, Mostoles, Spain},
  ISSN                     = {1382-3256},
  Issue                    = {1},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands},
  Url                      = {http://dx.doi.org/10.1007/s10664-011-9181-9}
}

@InProceedings{GrayBDSB11,
  Title                    = {The misuse of the NASA metrics data program data sets for automated software defect prediction},
  Author                   = {Gray, David and Bowes, David and Davey, Neil and Sun, Yi and Christianson, Bruce},
  Booktitle                = {Evaluation Assessment in Software Engineering (EASE 2011), 15th Annual Conference on},
  Year                     = {2011},
  Month                    = {april},
  Pages                    = {96 -103},

  Abstract                 = {Background: The NASA Metrics Data Program data sets have been heavily used in software defect prediction experiments. Aim: To demonstrate and explain why these data sets require significant pre-processing in order to be suitable for defect prediction. Method: A meticulously documented data cleansing process involving all 13 of the original NASA data sets. Results: Post our novel data cleansing process; each of the data sets had between 6 to 90 percent less of their original number of recorded values. Conclusions: One: Researchers need to analyse the data that forms the basis of their findings in the context of how it will be used. Two: Defect prediction data sets could benefit from lower level code metrics in addition to those more commonly used, as these will help to distinguish modules, reducing the likelihood of repeated data points. Three: The bulk of defect prediction experiments based on the NASA Metrics Data Program data sets may have led to erroneous findings. This is mainly due to repeated data points potentially causing substantial amounts of training and testing data to be identical.},
  Doi                      = {10.1049/ic.2011.0012}
}

@Article{HBBGC11,
  Title                    = {A Systematic Literature Review on Fault Prediction Performance in Software Engineering},
  Author                   = {Tracy Hall and Sarah Beecham and David Bowes and David Gray and Steve Counsell},
  Journal                  = {Transactions on Software Engineering},
  Year                     = {In Press -- 2011},

  Abstract                 = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs and
improve the quality of software.
Objective: We investigate how the context of models, the independent variables used and the modelling techniques applied, influence
the performance of fault prediction models.
Method:We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010.
We synthesise the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information
according to the criteria we develop and apply.
Results: The models that perform well tend to be based on simple modelling techniques such as Naöve Bayes or Logistic Regression.
Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these
combinations when models are performing particularly well.
Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault
prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their
context, methodology and performance comprehensively.},
  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@Article{Hand09,
  Title                    = {Measuring classifier performance: a coherent alternative to the area under the ROC curve},
  Author                   = {Hand, David J.},
  Journal                  = {Mach. Learn.},
  Year                     = {2009},

  Month                    = oct,
  Number                   = {1},
  Pages                    = {103--123},
  Volume                   = {77},

  Acmid                    = {1613009},
  Address                  = {Hingham, MA, USA},
  Doi                      = {10.1007/s10994-009-5119-5},
  ISSN                     = {0885-6125},
  Issue_date               = {October 2009},
  Keywords                 = {AUC, Classification, Cost, Error rate, Loss, Misclassification rate, ROC curves, Sensitivity, Specificity},
  Numpages                 = {21},
  Publisher                = {Kluwer Academic Publishers},
  Url                      = {http://dx.doi.org/10.1007/s10994-009-5119-5}
}

@InProceedings{Harman2010,
  Title                    = {The relationship between search based software engineering and predictive modeling},
  Author                   = {Harman, Mark},
  Booktitle                = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
  Year                     = {2010},

  Address                  = {New York, NY, USA},
  Pages                    = {1:1--1:13},
  Publisher                = {ACM},
  Series                   = {PROMISE '10},

  Acmid                    = {1868330},
  Articleno                = {1},
  Doi                      = {http://doi.acm.org/10.1145/1868328.1868330},
  ISBN                     = {978-1-4503-0404-7},
  Location                 = {Timisoara, Romania},
  Numpages                 = {13}
}

@InProceedings{HarmanC04,
  Title                    = {Metrics are fitness functions too},
  Author                   = {Harman, M. and Clark, J.},
  Booktitle                = {Proceedings. 10th International Symposium on Software Metrics 2004},
  Year                     = {2004},
  Month                    = {sept.},
  Pages                    = {58--69},

  Doi                      = {10.1109/METRIC.2004.1357891},
  ISSN                     = {1530-1435},
  Keywords                 = { fitness function; search-based software engineering; software metrics; software validation; genetic algorithms; program verification; software metrics; software process improvement;}
}

@Article{Harman2001,
  Title                    = {Search-based software engineering},
  Author                   = {Mark Harman and Bryan F. Jones},
  Journal                  = {Information and Software Technology},
  Year                     = {2001},
  Number                   = {14},
  Pages                    = {833--839},
  Volume                   = {43},

  ISSN                     = {0950-5849}
}

@Article{Hastings01,
  Title                    = {A vector-based approach to software size measurement and effort estimation},
  Author                   = {Hastings, T.E. and Sajeev, A.S.M.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2001},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {337--350},
  Volume                   = {27},

  Abstract                 = {Software size is a fundamental product measure that can be used for assessment, prediction and improvement purposes. However, existing software size measures, such as function points, do not address the underlying problem complexity of software systems adequately. This can result in disproportional measures of software size for different types of systems. We propose a vector size measure (VSM) that incorporates both functionality and problem complexity in a balanced and orthogonal manner. The VSM is used as the input to a vector prediction model (VPM) which can be used to estimate development effort early in the software life-cycle. We theoretically validate the approach against a formal framework. We also empirically validate the approach with a pilot study. The results indicate that the approach provides a mechanism to measure the size of software systems, classify software systems, and estimate development effort early in the software life-cycle to within plusmn;20% across a range of application types},
  Doi                      = {10.1109/32.917523},
  ISSN                     = {0098-5589},
  Keywords                 = {algebraic specification;application types;formal framework;functionality;gradient;magnitude;pilot study;problem complexity;semantic properties;software development effort estimation;software life-cycle;software metrics;software size measurement;software specification;software systems classification;syntactic properties;validation;vector prediction model;vector size measure;algebraic specification;computational complexity;size measurement;software cost estimation;software metrics;vectors;}
}

@Article{He_KDE09_Imbalance,
  Title                    = {Learning from Imbalanced Data},
  Author                   = {Haibo He and E.A. Garcia},
  Journal                  = {IEEE Transactions on Knowledge and Data Engineering},
  Year                     = {2009},

  Month                    = {Sept.},
  Number                   = {9},
  Pages                    = {1263--1284},
  Volume                   = {21},

  Abstract                 = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.},
  Doi                      = {10.1109/TKDE.2008.239},
  ISSN                     = {1041-4347},
  Keywords                 = {complex systems;data availability;data engineering;decision making;imbalanced data;knowledge discovery;large-scale systems;learning;networked systems;data mining;decision making;large-scale systems;learning (artificial intelligence);}
}

@Article{Heiat97,
  Title                    = {A model for estimating efforts required for developing small-scale business applications},
  Author                   = {Abbas Heiat and Nafisseh Heiat},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1997},
  Number                   = {1},
  Pages                    = {7--14},
  Volume                   = {39},

  Abstract                 = {Estimating the amount of effort required for developing an information system is an important project management concern. The author has developed a model that estimates small-scale software development effort in 4GL and end-user computing environments. In addition to presenting and evaluating the proposed model, this paper evaluates two of the most popular models currently used to estimate software development effort i.e., lines of code (LOC), and function points (FP). Results of the study show a significant correlation between the software development effort and all three models. Compared to LOC and FP models, the models developed in this research are less costly and easier to use in a small-scale software development environment.},
  Doi                      = {10.1016/S0164-1212(96)00159-8},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121296001598}
}

@InProceedings{herraiz2009:flossmetrics,
  Title                    = {{FLOSSMetrics}: Free / Libre / Open Source Software Metrics},
  Author                   = {Israel Herraiz and Daniel Izquierdo-Cortazar and Francisco Rivas-Hernandez and Jesus M. Gonzalez-Barahona and Gregorio Robles and Santiago Due\~nas Dominguez and Carlos Garcia-Campos and Juan Francisco Gato and Liliana Tovar},
  Booktitle                = {Proceedings of the 13th European Conference on Software Maintenance and Reengineering (CSMR)},
  Year                     = {2009},
  Publisher                = {IEEE Computer Society},

  Location                 = {Kaiserlauten, Germany}
}

@InProceedings{HRH_WETSOM12,
  Title                    = {On the Statistical Distribution of Object-Oriented System Properties},
  Author                   = {Israel Herraiz and Daniel Rodriguez and Rachel Harrison},
  Booktitle                = {3rd International Workshop on Emerging Trends in Software Metrics (WETSoM 2012)},
  Year                     = {2012},
  Month                    = {June},
  Publisher                = {IEEE}
}

@Article{Jorgensen2003,
  Title                    = {Software effort estimation by analogy and 'regression toward themean'},
  Author                   = {Magne J\o{}rgensen and Ulf Indahl and Dag Sj\o{}berg},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2003},
  Number                   = {3},
  Pages                    = {253--262},
  Volume                   = {68},

  Abstract                 = {Estimation by analogy is, simplified, the process of finding one or more projects that are similar to the one to be estimated and then derive the estimate from the values of these projects. If the selected projects have an unusual high or low productivity, then we should adjust the estimates toward productivity values of more average projects. The size of the adjustments depends on the expected accuracy of the estimation model. This paper evaluates one adjustment approach, based on the findings made by Sir Francis Galton in the late 1800s regarding the statistical phenomenon regression toward the mean (RTM). We evaluate this approach on several data sets and find indications that it improves the estimation accuracy. Surprisingly, current analogy based effort estimation models do not, as far as we know, include adjustments related to extreme analogues and inaccurate estimation models. An analysis of several industrial software development and maintenance projects indicates that the effort estimates provided by software professionals, i.e., expert estimates, to some extent are RTM-adjusted. A student experiment confirms this finding, but also indicates a rather large variance in how well the need for RTM-adjustments is understood among software developers.},
  Doi                      = {10.1016/S0164-1212(03)00066-9},
  ISSN                     = {0164-1212},
  Keywords                 = {Effort estimation},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121203000669}
}

@Article{Jorgensen07,
  Title                    = {A Systematic Review of Software Development Cost Estimation Studies},
  Author                   = {J\o{}rgensen, M. and Shepperd, M.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2007},

  Month                    = {jan.},
  Number                   = {1},
  Pages                    = {33--53},
  Volume                   = {33},

  Abstract                 = {This paper aims to provide a basis for the improvement of software-estimation research through a systematic review of previous work. The review identifies 304 software cost estimation papers in 76 journals and classifies the papers according to research topic, estimation approach, research approach, study context and data set. A Web-based library of these cost estimation papers is provided to ease the identification of relevant estimation research results. The review results combined with other knowledge provide support for recommendations for future software cost estimation research, including: 1) increase the breadth of the search for relevant studies, 2) search manually for relevant papers within a carefully selected set of journals when completeness is essential, 3) conduct more studies on estimation methods commonly used by the software industry, and 4) increase the awareness of how properties of the data sets impact the results when evaluating estimation methods},
  Doi                      = {10.1109/TSE.2007.256943},
  ISSN                     = {0098-5589}
}

@Article{Jeffery_ESE96,
  Title                    = {Function point sizing: Structure, validity and applicability},
  Author                   = {Jeffery, Ross and Stathis, John},
  Journal                  = {Empirical Software Engineering},
  Year                     = {1996},
  Note                     = {10.1007/BF00125809},
  Pages                    = {11-30},
  Volume                   = {1},

  Abstract                 = {This paper reports on a study carried out within a software development organization to evaluate the use of function points as a measure of early lifecycle software size. There were three major aims to the research: firstly to determine the extent to which the component elements of function points were independent of each other and thus appropriate for an additive model of size; secondly to investigate the relationship between effort and (1) the function point components, (2) unadjusted function points, and (3) adjusted function points, to determine whether the complexity weightings and technology adjustments were adding to the effort explanation power of the metric; and thirdly to investigate the suitability of function points for sizing in client server developments. The results show that the component parts are not independent of each other which supports an earlier study in this area. In addition the complexity weights and technology factors do not improve the effort/size model, suggesting that a simplified sizing metric may be appropriate. With respect to the third aim it was found that the function point metric revealed a much lower productivity in the client server environment. This likely is a reflection of cost of the introduction of newer technologies but is in need of further research.},
  ISSN                     = {1382-3256},
  Issue                    = {1},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands},
  Url                      = {http://dx.doi.org/10.1007/BF00125809}
}

@Article{Jorgensen04,
  Title                    = {Realism in assessment of effort estimation uncertainty: it matters how you ask},
  Author                   = {Jorgensen, M.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2004},

  Month                    = {april},
  Number                   = {4},
  Pages                    = {209--217},
  Volume                   = {30},

  Abstract                 = { Traditionally, software professionals are requested to provide minimum-maximum intervals to indicate the uncertainty of their effort estimates. We claim that the traditional request is not optimal and leads to overoptimistic views about the level of estimation uncertainty. Instead, we propose that it is better to frame the request for uncertainty assessment: "How likely is it that the actual effort will be more than/less than X?" Our claim is based on the results of a previously reported-experiment and field studies in two companies. The two software companies were instructed to apply the traditional and our alternative framing on random samples of their projects. In total, we collected information about 47 projects applying the traditional-framing and 23 projects applying the alternative framing.},
  Doi                      = {10.1109/TSE.2004.1274041},
  ISSN                     = {0098-5589},
  Keywords                 = { alternative framing project; risk assessment; software companies; software cost estimation; software professionals; software psychology; traditional-framing project; professional aspects; project management; risk management; software cost estimation; software houses;}
}

@Article{Kaymak_EAAI2012_AUK,
  Title                    = {The AUK: A simple alternative to the AUC},
  Author                   = {Uzay Kaymak and Arie Ben-David and Rob Potharst},
  Journal                  = {Engineering Applications of Artificial Intelligence},
  Year                     = {2012},
  Number                   = {5},
  Pages                    = {1082 - 1089},
  Volume                   = {25},

  Abstract                 = {The area under the receiver operating characteristic (ROC) curve, also known as the AUC-index, is commonly used for ranking the performance of data mining models. The AUC has various merits, such as ease of interpretation. However, since it is class indifferent, its usefulness while dealing with highly skewed data sets is questionable. In this paper, we propose a simple alternative scalar measure to the AUC-index, the area under the Kappa curve (AUK). The proposed AUK-index compensates for the class indifference of the AUC by being sensitive to the class distribution. Therefore, it is particularly suitable for measuring classifiers' performance on skewed data sets. After introducing the AUK we explore its mathematical relationship with the AUC and show that there is a non-linear relation between them.},
  Doi                      = {10.1016/j.engappai.2012.02.012},
  ISSN                     = {0952-1976},
  Keywords                 = {ROC curve},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0952197612000498}
}

@InProceedings{KeivanlooMSR12,
  Title                    = {A Linked Data platform for mining software repositories},
  Author                   = {Keivanloo, I. and Forbes, C. and Hmood, A. and Erfani, M. and Neal, C. and Peristerakis, G. and Rilling, J.},
  Booktitle                = {Mining Software Repositories (MSR), 2012 9th IEEE Working Conference on},
  Year                     = {2012},
  Month                    = {june},
  Pages                    = {32 -35},

  Abstract                 = {The mining of software repositories involves the extraction of both basic and value-added information from existing software repositories. The repositories will be mined to extract facts by different stakeholders (e.g. researchers, managers) and for various purposes. To avoid unnecessary pre-processing and analysis steps, sharing and integration of both basic and value-added facts are needed. In this research, we introduce SeCold, an open and collaborative platform for sharing software datasets. SeCold provides the first online software ecosystem Linked Data platform that supports data extraction and on-the-fly inter-dataset integration from major version control, issue tracking, and quality evaluation systems. In its first release, the dataset contains about two billion facts, such as source code statements, software licenses, and code clones from 18 000 software projects. In its second release the SeCold project will contain additional facts mined from issue trackers and versioning systems. Our approach is based on the same fundamental principle as Wikipedia: researchers and tool developers share analysis results obtained from their tools by publishing them as part of the SeCold portal and therefore make them an integrated part of the global knowledge domain. The SeCold project is an official member of the Linked Data dataset cloud and is currently the eighth largest online dataset available on the Web.},
  Doi                      = {10.1109/MSR.2012.6224296},
  ISSN                     = {2160-1852},
  Keywords                 = {code clones;collaborative platform;linked data platform;mining software repositories;on-the-fly inter-dataset integration;online software ecosystem linked data platform;software datasets;software licenses;software packages;software repositories;source code statements;value added information;data mining;software packages;}
}

@InProceedings{Keivanloo2011_SECOLD,
  Title                    = {Towards sharing source code facts using linked data},
  Author                   = {Keivanloo, Iman and Forbes, Christopher and Rilling, Juergen and Charland, Philippe},
  Booktitle                = {Proceedings of the 3rd International Workshop on Search-Driven Development: Users, Infrastructure, Tools, and Evaluation},
  Year                     = {2011},

  Address                  = {New York, NY, USA},
  Pages                    = {25--28},
  Publisher                = {ACM},
  Series                   = {SUITE '11},

  Acmid                    = {1985436},
  Doi                      = {10.1145/1985429.1985436},
  ISBN                     = {978-1-4503-0597-6},
  Keywords                 = {linked data, ontology, semantic web, source code model},
  Location                 = {Waikiki, Honolulu, HI, USA},
  Numpages                 = {4},
  Url                      = {http://doi.acm.org/10.1145/1985429.1985436}
}

@Article{Kemerer87,
  Title                    = {An empirical validation of software cost estimation models},
  Author                   = {Kemerer, Chris F},
  Journal                  = {Communications of the ACM},
  Year                     = {1987},

  Month                    = may,
  Number                   = {5},
  Pages                    = {416--429},
  Volume                   = {30},

  Acmid                    = {22906},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/22899.22906},
  ISSN                     = {0001-0782},
  Issue_date               = {May 1987},
  Numpages                 = {14},
  Publisher                = {ACM},
  Url                      = {http://doi.acm.org/10.1145/22899.22906}
}

@Misc{Keung11,
  Title                    = {Where is the Best Effort Estimator?},

  Author                   = {Jacky W. Keung and Ekrem Kocaguneli and Tim Menzies},

  Owner                    = {drg},
  Timestamp                = {2012.09.19},
  Url                      = {http://www.csee.wvu.edu/~timm/tmp/icse-v2.pdf}
}

@Article{Khoshgoftaar09,
  Title                    = {Empirical Case Studies in Attribute Noise Detection},
  Author                   = {Khoshgoftaar, T.M. and Van Hulse, J.},
  Journal                  = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {july },
  Number                   = {4},
  Pages                    = {379 -388},
  Volume                   = {39},

  Abstract                 = {The quality of data is an important issue in any domain-specific data mining and knowledge discovery initiative. The validity of solutions produced by data-driven algorithms can be diminished if the data being analyzed are of low quality. The quality of data is often realized in terms of data noise present in the given dataset and can include noisy attributes or labeling errors. Hence, tools for improving the quality of data are important to the data mining analyst. We present a comprehensive empirical investigation of our new and innovative technique for ranking attributes in a given dataset from most to least noisy. Upon identifying the noisy attributes, specific treatments can be applied depending on how the data are to be used. In a classification setting, for example, if the class label is determined to contain the most noise, processes to cleanse this important attribute may be undertaken. Independent variables or predictors that have a low correlation to the class attribute and appear noisy may be eliminated from the analysis. Several case studies using both real-world and synthetic datasets are presented in this study. The noise detection performance is evaluated by injecting noise into multiple attributes at different noise levels. The empirical results demonstrate conclusively that our technique provides a very accurate and useful ranking of noisy attributes in a given dataset.},
  Doi                      = {10.1109/TSMCC.2009.2013815},
  ISSN                     = {1094-6977},
  Keywords                 = {attribute noise detection;data quality;domain-specific data mining;knowledge discovery;data analysis;data mining;}
}

@InProceedings{KhoshgoftaarGKN12,
  Title                    = {Exploring an iterative feature selection technique for highly imbalanced data sets},
  Author                   = {Khoshgoftaar, Taghi M. and Gao, Kehan and Napolitano, Amri},
  Booktitle                = {2012 IEEE 13th International Conference on Information Reuse and Integration (IRI)},
  Year                     = {2012},
  Month                    = {aug.},
  Pages                    = {101--108},

  Abstract                 = {The quality of a classification model is affected by two factors in a training data set: (1) the presence of excessive features and (2) the presence of imbalanced distributions between two classes in a binary classification problem. This paper presents an iterative feature selection method to deal with these two problems. The proposed method consists of an iterative process of data sampling followed by feature ranking and finally aggregating the results generated during the iterative process. In this study, we investigate a number of feature ranking techniques and a data sampling method with two different post-sampling proportions between the two classes. We compare the iterative feature selection technique to the one where a data sampling and a feature ranking technique are used together but only once (without iteration). The empirical study is carried out on two groups of highly imbalanced data sets from a real-world software system. The results demonstrate that our proposed iterative feature selection technique performs on average better than the method without iteration.},
  Doi                      = {10.1109/IRI.2012.6302997}
}

@Article{KhoshgoftaarGN12,
  Title                    = {An Empirical Study of Feature Ranking Techniques for Software Quality Prediction},
  Author                   = {Taghi M. Khoshgoftaar and Kehan Gao and Amri Napolitano},
  Journal                  = {International Journal of Software Engineering and Knowledge Engineering},
  Year                     = {2012},
  Number                   = {2},
  Pages                    = {161-183},
  Volume                   = {22},

  Ee                       = {http://dx.doi.org/10.1142/S0218194012400013}
}

@InProceedings{KS02,
  Title                    = {Case and Feature Subset Selection in Case-Based Software Project Effort Prediction},
  Author                   = {C. Kirsopp and M. Shepperd},
  Booktitle                = {22nd International Conference on Knowledge-Based Systems and Applied Artificial Intelligence (SGAI'02)},
  Year                     = {2002}
}

@Article{springerlink:10.1007/s10664-008-9061-0,
  Title                    = {The role of replications in empirical software engineering: a word of warning},
  Author                   = {Kitchenham, Barbara},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2008},
  Number                   = {2},
  Pages                    = {219--221},
  Volume                   = {13},

  Affiliation              = {Keele University School of Computing and Mathematics Keele ST5 5BG UK},
  ISSN                     = {1382-3256},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands}
}

@Article{677185,
  Title                    = {A procedure for analyzing unbalanced datasets},
  Author                   = {Kitchenham, B.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {1998},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {278 -301},
  Volume                   = {24},

  Abstract                 = {This paper describes a procedure for analyzing unbalanced datasets that include many nominal- and ordinal-scale factors. Such datasets are often found in company datasets used for benchmarking and productivity assessment. The two major problems caused by lack of balance are that the impact of factors can be concealed and that spurious impacts can be observed. These effects are examined with the help of two small artificial datasets. The paper proposes a method of forward pass residual analysis to analyze such datasets. The analysis procedure is demonstrated on the artificial datasets and then applied to the COCOMO dataset. The paper ends with a discussion of the advantages and limitations of the analysis procedure},
  Doi                      = {10.1109/32.677185},
  ISSN                     = {0098-5589},
  Keywords                 = {COCOMO dataset;analysis of variance;benchmarking;forward pass residual analysis;productivity assessment;residual analysis;software metrics;statistical analysis;unbalanced datasets;software metrics;}
}

@Article{Kitchenham2002,
  Title                    = {The question of scale economies in software --- why cannot researchers agree?},
  Author                   = {Barbara A. Kitchenham},
  Journal                  = {Information and Software Technology},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {13--24},
  Volume                   = {44},

  Abstract                 = {This paper investigates the different research results obtained when different researchers have investigated the issue of economies and diseconomies of scale in software projects. Although researchers have used broadly similar sets of software project data sets, the results of their analyses and the conclusions they have drawn have differed. The paper highlights methodological differences that have lead to the conflicting results and shows how in many cases the differing results can be reconciled. It discusses the application of econometric concepts such as production frontiers and data envelopment analysis (DEA) to software data sets. It concludes that the assumptions underlying DEA may make it unsuitable for most software datasets but stochastic production frontiers may be relevant. It also raises some statistical issues that suggest testing hypothesis about economies and diseconomies of scale may be much more difficult than has been appreciated. The paper concludes with a plea for agreed standards for research synthesis activities.},
  Doi                      = {10.1016/S0950-5849(01)00204-X},
  ISSN                     = {0950-5849},
  Keywords                 = {Software estimation models},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S095058490100204X}
}

@Article{Kitchenham97,
  Title                    = {Evaluating software engineering methods and tools, part 7: planning feature analysis evaluation},
  Author                   = {Kitchenham, Barbara Ann},
  Journal                  = {SIGSOFT Software Engineering Notes},
  Year                     = {1997},

  Month                    = jul,
  Number                   = {4},
  Pages                    = {21--24},
  Volume                   = {22},

  Acmid                    = {263251},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/263244.263251},
  ISSN                     = {0163-5948},
  Issue_date               = {July 1997},
  Numpages                 = {4},
  Publisher                = {ACM},
  Url                      = {http://doi.acm.org/10.1145/263244.263251}
}

@TechReport{Kitchenham07,
  Title                    = {Guidelines for performing Systematic Literature Reviews in Software Engineering},
  Author                   = {Barbara A. Kitchenham and S. Charters},
  Institution              = {Keele University},
  Year                     = {2007},
  Number                   = {EBSE-2007-01},
  Type                     = {Technical Report},

  Source                   = {http://www.dur.ac.uk/ebse/guidelines.php}
}

@InProceedings{Kitchenham04,
  Title                    = {Evidence-Based Software Engineering},
  Author                   = {Barbara A. Kitchenham and Tore Dyba and Magne J{\o}rgensen},
  Booktitle                = {Proceedings of the 26th International Conference on Software Engineering (ICSE'04)},
  Year                     = {2004},

  Address                  = {Washington, DC, USA},
  Pages                    = {273--281},
  Publisher                = {IEEE Computer Society},

  ISBN                     = {0-7695-2163-0}
}

@Article{Kitchenham85,
  Title                    = {Software project development cost estimation},
  Author                   = {Barbara A. Kitchenham and N.R. Taylor},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1985},
  Number                   = {4},
  Pages                    = {267--278},
  Volume                   = {5},

  Abstract                 = {This paper reports the results of an empirical investigation of the relationships between effort expended, time scales, and project size for software project development. The observed relationships were compared with those predicted by Lawrence Putnam's Rayleigh curve model and Barry Boehm's COCOMO model. The results suggested that although the form of the basic empirical relationships were consistent with the cost models, the COCOMO model was a poor estimator of cost for the current data set and the data did not follow the Rayleigh curve suggested by Putnam. However, the results did suggest that it was possible to develop cost models tailored to a particular environment and to improve the precision of the models as they are used during the development cycle by including additional information such as the known effort for the early development phases. The paper finishes by discussing some of the problems involved in developing useful cost models.},
  Doi                      = {10.1016/0164-1212(85)90026-3},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0164121285900263}
}

@Article{Kitchenham02_CSC,
  Title                    = {An empirical study of maintenance and development estimation accuracy},
  Author                   = {Barbara Kitchenham and Shari Lawrence Pfleeger and Beth McColl and Suzanne Eagan},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {57--77},
  Volume                   = {64},

  Abstract                 = {We analyzed data from 145 maintenance and development projects managed by a single outsourcing company, including effort and duration estimates, effort and duration actuals, and function points counts. The estimates were made as part of the company's standard project estimating process that involved producing two or more estimates for each project and selecting one estimate to be the basis of client-agreed budgets. We found that effort estimates chosen as a basis for project budgets were, in general, reasonably good, with 63\% of the estimates being within 25\% of the actual value, and an average absolute error of 0.26. These estimates were significantly better than regression estimates based on adjusted function points, although the function point models were based on a homogeneous subset of the full data set, and we allowed for the fact that the model parameters changed over time. Furthermore, there was little evidence that the accuracy of the selected estimates was due to their becoming the target values for the project managers.},
  Doi                      = {10.1016/S0164-1212(02)00021-3},
  ISSN                     = {0164-1212},
  Keywords                 = {Estimation accuracy},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121202000213}
}

@Article{Kitchenham02_maintenance,
  Title                    = {An empirical study of maintenance and development estimation accuracy},
  Author                   = {Barbara Kitchenham and Shari Lawrence Pfleeger and Beth McColl and Suzanne Eagan},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {57--77},
  Volume                   = {64},

  Abstract                 = {We analyzed data from 145 maintenance and development projects managed by a single outsourcing company, including effort and duration estimates, effort and duration actuals, and function points counts. The estimates were made as part of the company's standard project estimating process that involved producing two or more estimates for each project and selecting one estimate to be the basis of client-agreed budgets. We found that effort estimates chosen as a basis for project budgets were, in general, reasonably good, with 63\% of the estimates being within 25\% of the actual value, and an average absolute error of 0.26. These estimates were significantly better than regression estimates based on adjusted function points, although the function point models were based on a homogeneous subset of the full data set, and we allowed for the fact that the model parameters changed over time. Furthermore, there was little evidence that the accuracy of the selected estimates was due to their becoming the target values for the project managers.},
  Doi                      = {10.1016/S0164-1212(02)00021-3},
  ISSN                     = {0164-1212},
  Keywords                 = {Estimation accuracy},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121202000213}
}

@Article{KKPJ10,
  Title                    = {Report from the 1st international workshop on replication in empirical software engineering research (RESER 2010)},
  Author                   = {Knutson, Charles D. and Krein, Jonathan L. and Prechelt, Lutz and Juristo, Natalia},
  Journal                  = {SIGSOFT Software Engineering Notes},
  Year                     = {2010},

  Month                    = {October},
  Pages                    = {42--44},
  Volume                   = {35},

  Acmid                    = {1838698},
  Address                  = {New York, NY, USA},
  Doi                      = {http://doi.acm.org/10.1145/1838687.1838698},
  ISSN                     = {0163-5948},
  Issue                    = {5},
  Keywords                 = {methods, replication, reporting, research, software engineering, validation, validity},
  Numpages                 = {3},
  Publisher                = {ACM}
}

@InProceedings{KJ95,
  Title                    = {Automatic Parameter Selection by Minimizing Estimated Error},
  Author                   = {R. Kohavi and G.H. John},
  Booktitle                = {12th Int. Conf. on Machine Learning},
  Year                     = {1995},

  Address                  = {San Francisco},
  Pages                    = {304--312}
}

@Article{Kohavi97,
  Title                    = {Data Mining using MLC++, a Machine Learning Library in C++},
  Author                   = {Ron Kohavi and Dan Sommerfield and James Dougherty},
  Journal                  = {International Journal of Artificial Intelligence Tools},
  Year                     = {1997},
  Number                   = {4},
  Pages                    = {537--566},
  Volume                   = {6}
}

@InProceedings{Li:2008:MDA:1370788.1370803,
  Title                    = {Multi-criteria decision analysis for customization of estimation by analogy method AQUA+},
  Author                   = {Li, Jingzhou and Ruhe, Guenther},
  Booktitle                = {Proceedings of the 4th international workshop on Predictor models in software engineering},
  Year                     = {2008},

  Address                  = {New York, NY, USA},
  Pages                    = {55--62},
  Publisher                = {ACM},
  Series                   = {PROMISE '08},

  Numpages                 = {8}
}

@InProceedings{LiR06,
  Title                    = {A comparative study of attribute weighting heuristics for effort estimation by analogy},
  Author                   = {Li, Jingzhou and Ruhe, Guenther},
  Booktitle                = {Proceedings of the 2006 ACM/IEEE international symposium on Empirical software engineering},
  Year                     = {2006},

  Address                  = {New York, NY, USA},
  Pages                    = {66--74},
  Publisher                = {ACM},
  Series                   = {ISESE '06},

  Acmid                    = {1159746},
  Doi                      = {10.1145/1159733.1159746},
  ISBN                     = {1-59593-218-6},
  Keywords                 = {attribute, estimation by analogy, missing values, non-quantitative attributes, rough set analysis, selection and weighting, software effort estimation},
  Location                 = {Rio de Janeiro, Brazil},
  Numpages                 = {9},
  Url                      = {http://doi.acm.org/10.1145/1159733.1159746}
}

@Article{LiRAR07,
  Title                    = {A flexible method for software effort estimation by analogy},
  Author                   = {Jingzhou Li and Guenther Ruhe and Ahmed Al-Emran and Michael M. Richter},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2007},
  Number                   = {1},
  Pages                    = {65--106},
  Volume                   = {12},

  Abstract                 = {Effort estimation by analogy uses information from former similar projects to predict the effort for a new project. Existing analogy-based methods are limited by their inability to handle non-quantitative data and missing values. The accuracy of predictions needs improvement as well. In this paper, we propose a new flexible method called AQUA that is able to overcome the limitations of former methods. AQUA combines ideas from two known analogy-based estimation techniques: case-based reasoning and collaborative filtering. The method is applicable to predict effort related to any object at the requirement, feature, or project levels. Which are the main contributions of AQUA when compared to other methods? First, AQUA supports non-quantitative data by defining similarity measures for different data types. Second, it is able to tolerate missing values. Third, the results from an explorative study in this paper shows that the prediction accuracy is sensitive to both the number N of analogies (similar objects) taken for adaptation and the threshold T for the degree of similarity, which is true especially for larger data sets. A fixed and small number of analogies, as assumed in existing analogy-based methods, may not produce the best accuracy of prediction. Fourth, a flexible mechanism based on learning of existing data is proposed for determining the appropriate values of N and T likely to offer the best accuracy of prediction. New criteria to measure the quality of prediction are proposed. AQUA was validated against two internal and one public domain data sets with non-quantitative attributes and missing values. The obtained results are encouraging. In addition, acomparative analysis with existing analogy-based estimation methods was conducted using three publicly available data sets that were used by these methods. Intwo of the three cases, AQUA outperformed all other methods.},
  Doi                      = {10.1007/s10664-006-7552-4},
  Url                      = {http://www.springerlink.com/content/5r0262nk47v15703}
}

@PhdThesis{Liebchen11_PhD,
  Title                    = {Data Cleaning Techniques for Software Engineering Data Sets},
  Author                   = {G. Liebchen},
  School                   = {Dept. of Information Systems and Computing, Brunel University},
  Year                     = {2011},

  Address                  = {London},
  Type                     = {PhD Dissertation}
}

@InProceedings{Lincke:2008,
  Title                    = {Comparing software metrics tools},
  Author                   = {Lincke, R\"{u}diger and Lundberg, Jonas and L\"{o}we, Welf},
  Booktitle                = {Proceedings of the 2008 International Symposium on Software Testing and Analysis (ISSTA'08)},
  Year                     = {2008},
  Pages                    = {131--142},
  Publisher                = {ACM},

  Numpages                 = {12}
}


@Article{LWHS91,
  Title                    = {Organizational benchmarking using the ISBSG Data Repository},
  Author                   = {Lokan, C. and Wright, T. and Hill, P. and Stringer, M.},
  Journal                  = {IEEE Software},
  Year                     = {2001},

  Month                    = {sep/oct},
  Number                   = {5},
  Pages                    = {26 -32},
  Volume                   = {18},

  Doi                      = {10.1109/52.951491},
  ISSN                     = {0740-7459},
  Keywords                 = {ISBSG Data Repository;International Software Benchmarking Standards Group;best-practice networking;completed software projects;cost estimation;function points structure;organizational benchmarking;project benchmarking;project duration;summary analyses;software performance evaluation;software standards;}
}

@Misc{Lopes+Bajracharya+Ossher+Baldi:2010,
  Title                    = {{UCI} Source Code Data Sets},

  Author                   = {C. Lopes and S. Bajracharya and J. Ossher and P. Baldi},
  Year                     = {2010},

  Institution              = {University of California, Irvine, Bren School of Information and Computer Sciences},
  Url                      = {http://www.ics.uci.edu/$\sim$lopes/datasets/}
}

@Article{MairSJ05,
  Title                    = {An analysis of data sets used to train and validate cost prediction systems},
  Author                   = {Mair, Carolyn and Shepperd, Martin and J{\o}rgensen, Magne},
  Journal                  = {SIGSOFT Software Engineering Notes},
  Year                     = {2005},

  Month                    = may,
  Number                   = {4},
  Pages                    = {1--6},
  Volume                   = {30},

  Acmid                    = {1083166},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/1082983.1083166},
  ISSN                     = {0163-5948},
  Issue_date               = {July 2005},
  Keywords                 = {cost prediction, data sets, standardisation},
  Numpages                 = {6},
  Publisher                = {ACM},
  Url                      = {http://doi.acm.org/10.1145/1082983.1083166}
}

@Article{Matson,
  Title                    = {Software development cost estimation using function points},
  Author                   = {Matson, J.E. and Barrett, B.E. and Mellichamp, J.M.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {1994},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {275 -287},
  Volume                   = {20},

  Abstract                 = {This paper presents an assessment of several published statistical regression models that relate software development effort to software size measured in function points. The principal concern with published models has to do with the number of observations upon which the models were based and inattention to the assumptions inherent in regression analysis. The research describes appropriate statistical procedures in the context of a case study based on function point data for 104 software development projects and discusses limitations of the resulting model in estimating development effort. The paper also focuses on a problem with the current method for measuring function points that constrains the effective use of function points in regression models and suggests a modification to the approach that should enhance the accuracy of prediction models based on function points in the future},
  Doi                      = {10.1109/32.277575},
  ISSN                     = {0098-5589},
  Keywords                 = {development effort;function points;prediction models;regression analysis;software development cost estimation;statistical regression models;software cost estimation;statistical analysis;}
}

@Book{Maxwell02,
  Title                    = {Applied statistics for software managers},
  Author                   = {Katrina Maxwell},
  Publisher                = {Prentice Hall},
  Year                     = {2002},

  Pages                    = {333}
}

@InProceedings{Mende2010,
  Title                    = {Replication of defect prediction studies: problems, pitfalls and recommendations},
  Author                   = {Mende, Thilo},
  Booktitle                = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
  Year                     = {2010},

  Address                  = {New York, NY, USA},
  Pages                    = {5:1--5:10},
  Publisher                = {ACM},
  Series                   = {PROMISE'10},

  Acmid                    = {1868336},
  Articleno                = {5},
  Doi                      = {10.1145/1868328.1868336},
  ISBN                     = {978-1-4503-0404-7},
  Keywords                 = {defect prediction model, replication},
  Location                 = {Timisoara, Romania},
  Numpages                 = {10},
  Url                      = {http://doi.acm.org/10.1145/1868328.1868336}
}

@InProceedings{Mende10,
  Title                    = {Effort-Aware Defect Prediction Models},
  Author                   = {Mende, Thilo and Koschke, Rainer},
  Booktitle                = {Proceedings of the 2010 14th European Conference on Software Maintenance and Reengineering (CSMR'10)},
  Year                     = {2010},

  Address                  = {Washington, DC, USA},
  Pages                    = {107--116},
  Publisher                = {IEEE Computer Society},
  Series                   = {CSMR'10},

  Acmid                    = {1955974},
  Doi                      = {10.1109/CSMR.2010.18},
  ISBN                     = {978-0-7695-4321-5},
  Keywords                 = {Defect Prediction Models, Evaluation, Cost-Benefits},
  Numpages                 = {10},
  Url                      = {http://dx.doi.org/10.1109/CSMR.2010.18}
}

@InProceedings{Mende09,
  Title                    = {Revisiting the evaluation of defect prediction models},
  Author                   = {Thilo Mende and Rainer Koschke},
  Booktitle                = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering (PROMISE'09)},
  Year                     = {2009},

  Address                  = {New York, NY, USA},
  Pages                    = {1--10},
  Publisher                = {ACM},

  Doi                      = {http://doi.acm.org/10.1145/1540438.1540448},
  ISBN                     = {978-1-60558-634-2},
  Location                 = {Vancouver, British Columbia, Canada}
}

@Article{MendesMDG08,
  Title                    = {Cross-company vs. single-company web effort models using the Tukutuku database: An extended study},
  Author                   = {Emilia Mendes and Sergio {Di Martino} and Filomena Ferrucci and Carmine Gravino},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2008},
  Number                   = {5},
  Pages                    = {673 - 690},
  Volume                   = {81},

  Doi                      = {10.1016/j.jss.2007.07.044},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121207002385}
}

@InProceedings{Menzies11ASE,
  Title                    = {Local vs. global models for effort estimation and defect prediction},
  Author                   = {Menzies, T. and Butcher, A. and Marcus, A. and Zimmermann, T. and Cok, D.},
  Booktitle                = {Automated Software Engineering (ASE), 2011 26th IEEE/ACM International Conference on},
  Year                     = {2011},
  Pages                    = {343--351},

  Doi                      = {10.1109/ASE.2011.6100072},
  ISSN                     = {1938-4300},
  Keywords                 = {data mining;program diagnostics;conclusion instability;data heterogeneity;data mining;effort estimation;global models;local models;software module defect prediction;Context;Couplings;Estimation;Principal component analysis;Runtime;Software;USA Councils;Data mining;defect/effort estimation;empirical SE;validation}
}

@Misc{promise12,
  Title                    = {The PROMISE Repository of empirical software engineering data},

  Author                   = {Tim Menzies and Bora Caglayan and Ekrem Kocaguneli and Joe Krall and Fayola Peters and Burak Turhan },
  Month                    = {June},
  Year                     = {2012},

  Url                      = {http://promisedata.googlecode.com}
}

@Article{Menzies07,
  Title                    = {Data Mining Static Code Attributes to Learn Defect Predictors},
  Author                   = {T. Menzies and J. Greenwald and A. Frank},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2007},

  Optmonth                 = {January},
  Optnumber                = {1},
  Optpages                 = {2--13},
  Optvolume                = {33},
  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@Article{Misic19981,
  Title                    = {Estimation of effort and complexity: An object-oriented case study},
  Author                   = {Vojislav B Mi\v{s}i\'c and Dejan N Tev{s}i\'c},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1998},
  Number                   = {2},
  Pages                    = {133--143},
  Volume                   = {41},

  Abstract                 = {The metrication of object-oriented software systems is still an underdeveloped part within the domain of the object paradigm. An empirical investigation aimed at finding appropriate measures and establishing simple, yet usable and cost-effective models for estimation and control of object-oriented system projects, was undertaken on a set of object-oriented projects implemented in a stable environment. First, the measures available were screened for possible correlations; then, the models suitable for estimation were derived and discussed. Effort was found to correlate well with the total number of classes and the total number of methods, both of which are known at the end of the design phase. A number of other models for estimation of the source code complexity were also defined.},
  Doi                      = {10.1016/S0164-1212(97)10014-0},
  ISSN                     = {0164-1212},
  Keywords                 = {Software measurement},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121297100140}
}

@Article{Mikut11_DMReview,
  Title                    = {Data mining tools},
  Author                   = {Mikut, Ralf and Reischl, Markus},
  Journal                  = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  Year                     = {2011},
  Number                   = {5},
  Pages                    = {431--443},
  Volume                   = {1},

  Doi                      = {10.1002/widm.24},
  ISSN                     = {1942-4795},
  Publisher                = {John Wiley \& Sons, Inc.},
  Url                      = {http://dx.doi.org/10.1002/widm.24}
}

@Article{Miyazaki94,
  Title                    = {Robust regression for developing software estimation models},
  Author                   = {Y. Miyazaki and M. Terakado and K. Ozaki and H. Nozaki},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1994},
  Number                   = {1},
  Pages                    = {3--16},
  Volume                   = {27},

  Abstract                 = {To develop a good software estimation model fitted to actual data, the evaluation criteria of goodness of fit is necessary. The first major problem discussed here is that ordinary relative error used for this criterion is not suitable because it has a bound in the case of under-estimation and no bound in the case of overestimation. We propose use of a new relative error called balanced relative error as the basis for the criterion and introduce seven evaluation criteria for software estimation models. The second major problem is that the ordinary least-squares method used for calculation of parameter values of a software estimation model is neither consistent with the criteria nor robust enough, which means that the solution is easily distorted by outliers. We propose a new consistent and robust method called the least-squares of inverted balanced relative errors (LIRS) and demonstrates its superiority to the ordinary least-squares method by use of five actual data sets. Through the analysis of these five data sets with LIRS, we show the importance of consistent data collection and development standarization to develop a good software sizing model. We compare goodness of fit between the sizing model based on the number of screens, forms, and files, and the sizing model based on the number of data elements for each of them. Based on this comparison, the validity of the number of data elements as independent variables for a sizing model is examined. Moreover, the validity of increasing the number of independent variables is examined.},
  Doi                      = {10.1016/0164-1212(94)90110-4},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0164121294901104}
}

@Article{mockus2002,
  Title                    = {Two case studies of {O}pen {S}ource Software development: {A}pache and {M}ozilla},
  Author                   = {Audris Mockus and Roy T. Fielding and James D. Herbsleb},
  Journal                  = {ACM Transactions on Software Engineering and Methodology},
  Year                     = {2002},
  Number                   = {3},
  Pages                    = {309--346},
  Volume                   = {11}
}

@Article{Moser1999,
  Title                    = {Cost estimation based on business models},
  Author                   = {Simon Moser and Brian Henderson-Sellers and Vojislav B Mi\v{s}i\'c},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1999},
  Number                   = {1},
  Pages                    = {33--42},
  Volume                   = {49},

  Abstract                 = {Software development requires early and accurate cost estimation in order to enhance likely success. System complexity needs to be measured and then correlated with development effort. One of the best known approaches to such measurement-based estimation in the area of Information Systems is Function Point Analysis (FPA). Although it is reasonably well used in practice, FPA has been shown to be formally ambiguous and to have some serious practical deficiencies as well, mainly in the context of newly emerged object-oriented modeling approaches. This paper reports results from an empirical study undertaken in Swiss industry covering 36 projects. We observed that a new formally sound approach, the System Meter (SM) method, which explicitly takes reuse into account, predicts effort substantially better than FPA.},
  Doi                      = {10.1016/S0164-1212(99)00064-3},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121299000643}
}

@Article{MyrtveitTSE05,
  Title                    = {Reliability and validity in comparative studies of software prediction models},
  Author                   = {Myrtveit, I. and Stensrud, E. and Shepperd, M.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2005},

  Month                    = {may},
  Number                   = {5},
  Pages                    = { 380--391},
  Volume                   = {31},

  Abstract                 = { Empirical studies on software prediction models do not converge with respect to the question "which prediction model is best?" The reason for this lack of convergence is poorly understood. In this simulation study, we have examined a frequently used research procedure comprising three main ingredients: a single data sample, an accuracy indicator, and cross validation. Typically, these empirical studies compare a machine learning model with a regression model. In our study, we use simulation and compare a machine learning and a regression model. The results suggest that it is the research procedure itself that is unreliable. This lack of reliability may strongly contribute to the lack of convergence. Our findings thus cast some doubt on the conclusions of any study of competing software prediction models that used this research procedure as a basis of model comparison. Thus, we need to develop more reliable research procedures before we can have confidence in the conclusions of comparative studies of software prediction models.},
  Doi                      = {10.1109/TSE.2005.58},
  ISSN                     = {0098-5589},
  Keywords                 = { accuracy indicator; analogy estimation; arbitrary function approximators; convergence; cost estimation; cross validation; data sample; empirical method; machine learning model; regression model; simulation; software metrics; software prediction model; software reliability; software validity; convergence; function approximation; learning (artificial intelligence); program verification; regression analysis; software cost estimation; software metrics; software reliability;}
}

@InProceedings{NZZH10,
  Title                    = {Change Bursts as Defect Predictors},
  Author                   = {Nachiappan Nagappan and Andreas Zeller and Thomas Zimmermann and Kim Herzig and Brendan Murphy},
  Booktitle                = {Proceedings of the 21st IEEE International Symposium on Software Reliability Engineering (ISSRE 2012)},
  Year                     = {2010},
  Month                    = {November},

  Location                 = {San Jose, California, USA}
}

@InProceedings{NZ1,
  Title                    = {The Ultimate Debian Database: Consolidating bazaar metadata for Quality Assurance and data mining},
  Author                   = {Nussbaum, L. and Zacchiroli, S.},
  Booktitle                = {7th IEEE Working Conference on Mining Software Repositories (MSR 2010)},
  Year                     = {2010},
  Month                    = {May},
  Pages                    = {52--61},

  Doi                      = {10.1109/MSR.2010.5463277},
  Keywords                 = {Debian project;FLOSS distribution;FLOSS project;RedHat;SQL database;SQL query;Ubuntu;bazaar development model;bazaar metadata;community driven distribution;data mining;open source software;quality assurance;SQL;data mining;marketing data processing;meta data;public domain software;quality assurance;}
}

@Misc{ohloh12,
  Title                    = {Ohloh},

  Author                   = {Ohloh},

  Url                      = {http://www.ohloh.net/}
}

@Article{Perini_TSE12,
  Title                    = {A Machine Learning Approach to Software Requirements Prioritization},
  Author                   = {Perini, A. and Susi, A. and Avesani, P.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2012},
  Number                   = {--},
  Pages                    = {--},
  Volume                   = {Preprint},

  Abstract                 = {Deciding which, among a set of requirements, are to be considered first and in which order, is a strategic process in software development. This task is commonly referred as requirements prioritization. This paper describes a requirements prioritization method, called CBRank, which combines project's stakeholders preferences with requirements ordering approximations computed through Machine Learning techniques, bringing promising advantages. First, the human effort to input preference information can be reduced, while preserving the accuracy of the final ranking estimates. Second, domain knowledge encoded as partial order relations defined over the requirement attributes can be exploited, thus supporting an adaptive elicitation process. The techniques CBRank rests on and the associated prioritization process are detailed. Empirical evaluations of properties of CBRank are performed on simulated data and compared with a state-of-the art prioritization method, providing evidence of the method ability to support the management of the trade-off between elicitation effort and ranking accuracy, and to exploit domain knowledge. A case study on a real software project complement these experimental measurements. Finally, a positioning of CBRank with respect to state-of-the-art requirements prioritization methods is proposed, together with a discussion of benefit and limits of the method.},
  Doi                      = {10.1109/TSE.2012.52},
  ISSN                     = {0098-5589},
  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@Article{Raiha10,
  Title                    = {A Survey on Search-based Software Design},
  Author                   = {Outi R\"aih\"a},
  Journal                  = {Computer Science Review},
  Year                     = {2010},

  Month                    = {November},
  Number                   = {4},
  Pages                    = {203--249},
  Volume                   = {4},

  Doi                      = {http://dx.doi.org/10.1016/j.cosrev.2010.06.001},
  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@InProceedings{Rakotomalala05,
  Title                    = {TANAGRA : un logiciel gratuit pour l'enseignement et la recherche},
  Author                   = {Ricco Rakotomalala},
  Booktitle                = {Actes de EGC'2005, RNTI-E-3},
  Year                     = {2005},
  Pages                    = {697--702},
  Volume                   = {2}
}

@InProceedings{Ratzinger07_Refactoring,
  Title                    = {Mining Software Evolution to Predict Refactoring},
  Author                   = {Ratzinger, J. and Sigmund, T. and Vorburger, P. and Gall, H.},
  Booktitle                = {Empirical Software Engineering and Measurement, 2007. ESEM 2007. First International Symposium on},
  Year                     = {2007},
  Month                    = {sept.},
  Pages                    = {354 -363},

  Abstract                 = {Can we predict locations of future refactoring based on the development history? In an empirical study of open source projects we found that attributes of software evolution data can be used to predict the need for refactoring in the following two months of development. Information systems utilized in software projects provide a broad range of data for decision support. Versioning systems log each activity during the development, which we use to extract data mining features such as growth measures, relationships between classes, the number of authors working on a particular piece of code, etc. We use this information as input into classification algorithms to create prediction models for future refactoring activities. Different state-of-the-art classifiers are investigated such as decision trees, logistic model trees, prepositional rule learners, and nearest neighbor algorithms. With both high precision and high recall we can assess the refactoring proneness of object-oriented systems. Although we investigate different domains, we discovered critical factors within the development life cycle leading to refactoring, which are common among all studied projects.},
  Doi                      = {10.1109/ESEM.2007.9},
  ISSN                     = {1938-6451},
  Keywords                 = {data mining features;decision support;decision trees;development history;logistic model trees;mining software evolution;object-oriented systems;open source projects;prepositional rule learners;software projects;versioning systems;data mining;decision trees;object-oriented methods;software engineering;},
  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@Article{RaudysJ91,
  Title                    = {Small sample size effects in statistical pattern recognition: recommendations for practitioners},
  Author                   = {Raudys, S.J. and Jain, A.K.},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {1991},

  Month                    = {mar},
  Number                   = {3},
  Pages                    = {252--264},
  Volume                   = {13},

  Doi                      = {10.1109/34.75512},
  ISSN                     = {0162-8828},
  Keywords                 = {classifiers;error estimation;error rates;feature selection;learning;sample size effects;statistical pattern recognition;pattern recognition;statistical analysis;}
}

@InProceedings{Rob10,
  Title                    = {Replicating {MSR}: A study of the potential replicability of papers published in the Mining Software Repositories proceedings},
  Author                   = {Robles, G.},
  Booktitle                = {7th IEEE Working Conference on Mining Software Repositories (MSR 2010)},
  Year                     = {2010},
  Month                    = {may},
  Pages                    = {171--180},

  Doi                      = {10.1109/MSR.2010.5463348},
  Keywords                 = {MSR;MSR replication;data sources;mining software repositories proceedings;potential replicability;software projects;data mining;}
}

@Article{RGIH09,
  Title                    = {Tools for the study of the usual data sources found in libre software projects},
  Author                   = {Gregorio Robles and Jesus M. Gonzalez-Barahona and Daniel Izquierdo-Cortazar and Israel Herraiz},
  Journal                  = {International Journal of Open Source Software and Processes},
  Year                     = {2009},

  Month                    = {Jan-March},
  Number                   = {1},
  Pages                    = {24--45},
  Volume                   = {1}
}

@InProceedings{RRCA07,
  Title                    = {Detecting Fault Modules Applying Feature Selection to Classifiers},
  Author                   = {Rodriguez, D. and Ruiz, R. and Cuadrado-Gallego, J. and Aguilar-Ruiz, J.},
  Booktitle                = {Information Reuse and Integration, 2007. IRI 2007. IEEE International Conference on},
  Year                     = {2007},
  Month                    = {aug.},
  Pages                    = {667--672},

  Doi                      = {10.1109/IRI.2007.4296696},
  Keywords                 = {PROMISE repository;attribute selection techniques;automated data collection tools;classifier learning;data mining algorithms;fault module detection;feature selection;project management;software engineering databases;data mining;feature extraction;learning (artificial intelligence);pattern classification;project management;software management;}
}

@PhdThesis{Schofield98PhD,
  Title                    = {An Empirical investigation into software effort estimation by analogy},
  Author                   = {C. Schofield},
  School                   = {Bournemouth University},
  Year                     = {1998},

  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@Article{Shang2012,
  Title                    = {Using {PIG} as a data preparation language for large-scale mining software repositories studies: An experience report},
  Author                   = {Weiyi Shang and Bram Adams and Ahmed E. Hassan},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2012},
  Number                   = {10},
  Pages                    = {2195--2204},
  Volume                   = {85},

  Abstract                 = {The Mining Software Repositories (MSR) field analyzes software repository data to uncover knowledge and assist development of ever growing, complex systems. However, existing approaches and platforms for MSR analysis face many challenges when performing large-scale MSR studies. Such approaches and platforms rarely scale easily out of the box. Instead, they often require custom scaling tricks and designs that are costly to maintain and that are not reusable for other types of analysis. We believe that the web community has faced many of these software engineering scaling challenges before, as web analyses have to cope with the enormous growth of web data. In this paper, we report on our experience in using a web-scale platform (i.e., Pig) as a data preparation language to aid large-scale MSR studies. Through three case studies, we carefully validate the use of this web platform to prepare (i.e., Extract, Transform, and Load, ETL) data for further analysis. Despite several limitations, we still encourage MSR researchers to leverage Pig in their large-scale studies because of Pig's scalability and flexibility. Our experience report will help other researchers who want to scale their analyses.},
  Doi                      = {10.1016/j.jss.2011.07.034},
  ISSN                     = {0164-1212},
  Keywords                 = {Software engineering},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121211002007}
}

@InProceedings{Shang:2010:ERS:1858996.1859050,
  Title                    = {An experience report on scaling tools for mining software repositories using MapReduce},
  Author                   = {Shang, Weiyi and Adams, Bram and Hassan, Ahmed E.},
  Booktitle                = {Proceedings of the IEEE/ACM international conference on Automated software engineering},
  Year                     = {2010},

  Address                  = {New York, NY, USA},
  Pages                    = {275--284},
  Publisher                = {ACM},
  Series                   = {ASE '10},

  Acmid                    = {1859050},
  Doi                      = {10.1145/1858996.1859050},
  ISBN                     = {978-1-4503-0116-9},
  Keywords                 = {cloud computing, mapreduce, mining software repositories},
  Location                 = {Antwerp, Belgium},
  Numpages                 = {10},
  Url                      = {http://doi.acm.org/10.1145/1858996.1859050}
}

@Article{Shepperd_TSE01,
  Title                    = {Predicting with sparse data},
  Author                   = {Shepperd, M. and Cartwright, M.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {nov},
  Number                   = {11},
  Pages                    = {987 -998},
  Volume                   = {27},

  Abstract                 = {It is well-known that effective prediction of project cost related factors is an important aspect of software engineering. Unfortunately, despite extensive research over more than 30 years, this remains a significant problem for many practitioners. A major obstacle is the absence of reliable and systematic historic data, yet this is a sine qua non for almost all proposed methods: statistical, machine learning or calibration of existing models. The authors describe our sparse data method (SDM) based upon a pairwise comparison technique and T.L. Saaty's (1980) Analytic Hierarchy Process (AHP). Our minimum data requirement is a single known point. The technique is supported by a software tool known as DataSalvage. We show, for data from two companies, how our approach, based upon expert judgement, adds value to expert judgement by producing significantly more accurate and less biased results. A sensitivity analysis shows that our approach is robust to pairwise comparison errors. We then describe the results of a small usability trial with a practicing project manager. From this empirical work, we conclude that the technique is promising and may help overcome some of the present barriers to effective project prediction},
  Doi                      = {10.1109/32.965339},
  ISSN                     = {0098-5589},
  Keywords                 = {AHP;Analytic Hierarchy Process;DataSalvage;SDM;expert judgement;minimum data requirement;pairwise comparison errors;pairwise comparison technique;practicing project manager;project cost related factor prediction;project prediction;sensitivity analysis;single known point;software engineering;software project effort;software tool;sparse data;sparse data method;systematic historic data;usability trial;data analysis;software cost estimation;software reliability;software tools;}
}

@Article{SK01,
  Title                    = {Comparing software prediction techniques using simulation},
  Author                   = {Shepperd, M. and Kadoda, G.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2001},

  Month                    = {nov},
  Number                   = {11},
  Pages                    = {1014--1022},
  Volume                   = {27},

  Doi                      = {10.1109/32.965341},
  ISSN                     = {0098-5589},
  Keywords                 = {case-based reasoning;data set characteristics;machine learning;nearest neighbor;neural nets;prediction problem;regression;rule induction;simulation;small data sets;software prediction systems;software prediction technique comparison;training set;case-based reasoning;learning (artificial intelligence);neural nets;software metrics;virtual machines;}
}

@Article{Shepperd2012,
  Title                    = {Evaluating prediction systems in software project estimation},
  Author                   = {Martin Shepperd and Steve MacDonell},
  Journal                  = {Information and Software Technology},
  Year                     = {2012},
  Pages                    = {-},

  Doi                      = {10.1016/j.infsof.2011.12.008},
  ISSN                     = {0950-5849}
}

@Article{Shepperd97_Analogy,
  Title                    = {Estimating software project effort using analogies},
  Author                   = {Shepperd, M. and Schofield, C.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {1997},

  Month                    = {nov},
  Number                   = {11},
  Pages                    = {736 -743},
  Volume                   = {23},

  Abstract                 = {Accurate project effort prediction is an important goal for the software engineering community. To date most work has focused upon building algorithmic models of effort, for example COCOMO. These can be calibrated to local environments. We describe an alternative approach to estimation based upon the use of analogies. The underlying principle is to characterize projects in terms of features (for example, the number of interfaces, the development method or the size of the functional requirements document). Completed projects are stored and then the problem becomes one of finding the most similar projects to the one for which a prediction is required. Similarity is defined as Euclidean distance in n-dimensional space where n is the number of project features. Each dimension is standardized so all dimensions have equal weight. The known effort values of the nearest neighbors to the new project are then used as the basis for the prediction. The process is automated using a PC-based tool known as ANGEL. The method is validated on nine different industrial datasets (a total of 275 projects) and in all cases analogy outperforms algorithmic models based upon stepwise regression. From this work we argue that estimation by analogy is a viable technique that, at the very least, can be used by project managers to complement current estimation techniques},
  Doi                      = {10.1109/32.637387},
  ISSN                     = {0098-5589},
  Keywords                 = {ANGEL;COCOMO;Euclidean distance;algorithmic models;estimation by analogy;functional requirements document;industrial datasets;nearest neighbors;personal computer-based tool;project effort prediction;project management;software development method;software engineering;software project effort estimation;stepwise regression;project management;software cost estimation;software development management;software metrics;software tools;}
}

@Article{daSilva_ESE12,
  Title                    = {Replication of empirical studies in software engineering research: a systematic mapping study},
  Author                   = {Fabio Q. B. da Silva and Marcos Suassuna and A. Cà€Œsar C. Franà¥a and Alicia M. Grubb and Tatiana B. Gouveia and Cleviton V. F. Monteiro and Igor Ebrahim dos Santos},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2012}
}

@InProceedings{QualitasCorpus:APSEC:2010,
  Title                    = {Qualitas Corpus: A Curated Collection of Java Code for Empirical Studies},
  Author                   = {Tempero, Ewan and Anslow, Craig and Dietrich, Jens and Han, Ted and Li, Jing and Lumpe, Markus and Melton, Hayden and Noble, James},
  Booktitle                = {2010 Asia Pacific Software Engineering Conference (APSEC2010)},
  Year                     = {2010},
  Month                    = {Dec}
}

@InProceedings{Thai-Nghe11,
  Title                    = {A new evaluation measure for learning from imbalanced data},
  Author                   = {Nguyen Thai-Nghe and Gantner, Z. and Schmidt-Thieme, L.},
  Booktitle                = {The 2011 International Joint Conference on Neural Networks (IJCNN'11)},
  Year                     = {2011},
  Month                    = {31 2011-aug. 5},
  Pages                    = {537 -542},

  Abstract                 = {Recently, researchers have shown that the Area Under the ROC Curve (AUC) has a serious deficiency since it implicitly uses different misclassification cost distributions for different classifiers. Thus, using the AUC can be compared to using different metrics to evaluate different classifiers [1]. To overcome this incoherence, the H measure was proposed, which uses a symmetric Beta distribution to replace the implicit cost weight distribution in the AUC. When learning from imbalanced data, misclassifying a minority class example is much more serious than misclassifying a majority class example. To take different misclassification costs into account, we propose using an asymmetric Beta distribution (B42) instead of a symmetric one. Experimental results on 36 imbalanced data sets using SVMs and logistic regression show that B42 is a good choice for evaluating on imbalanced data sets because it puts more weight on the minority class. We also show that balanced random undersampling does not work for large and highly imbalanced data sets, although it has been reported to be effective for small data sets.},
  Doi                      = {10.1109/IJCNN.2011.6033267},
  ISSN                     = {2161-4393},
  Keywords                 = {AUC;Area Under the ROC Curve;Beta distribution;H measuremenht;SVM;imbalanced data learning;logistic regression;support vector machine;data handling;learning (artificial intelligence);}
}

@Article{TurhanESE12,
  Title                    = {On the dataset shift problem in software engineering prediction models},
  Author                   = {Turhan, Burak},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2012},
  Note                     = {10.1007/s10664-011-9182-8},
  Pages                    = {62--74},
  Volume                   = {17},

  Affiliation              = {Department of Information Processing Science, University of Oulu, POB.3000, 90014 Oulu, Finland},
  ISSN                     = {1382-3256},
  Issue                    = {1},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands}
}

@Conference{Van-Antwerp:2008,
  Title                    = {Advances in the SourceForge Research Data Archive (SRDA)},
  Author                   = {Van Antwerp, M. and Madey, G.},
  Booktitle                = {Fourth International Conference on Open Source Systems, IFIP 2.13 (WoPDaSD 2008)},
  Year                     = {2008},

  Address                  = {Milan, Italy},
  Month                    = {September}
}

@PhdThesis{Vasa2010,
  Title                    = {Growth and Change Dynamics in Open Source Software Systems},
  Author                   = {Rajesh Vasa},
  School                   = {Faculty of Information and Communication Technologies Swinburne University of Technology Melbourne, Australia},
  Year                     = {2010}
}

@Misc{Helix10a,
  Title                    = {Helix - Software Evolution Data Set},

  Author                   = {Rajesh Vasa and Markus Lumpe and and Allan Jones},
  HowPublished             = {\url{http://http://www.ict.swin.edu.au/research/projects/helix}},
  Year                     = {2010},

  Url                      = {http://www.ict.swin.edu.au/research/projects/helix}
}

@InProceedings{WangKWN12,
  Title                    = {A Comparative Study on the Stability of Software Metric Selection Techniques},
  Author                   = {Huanjing Wang and Taghi M. Khoshgoftaar and Randall Wald and Amri Napolitano},
  Booktitle                = {11th International Conference on Machine Learning and Applications, ICMLA},
  Year                     = {2012},
  Pages                    = {301--307},

  Ee                       = {http://dx.doi.org/10.1109/ICMLA.2012.142}
}

@InProceedings{Weimer09,
  Title                    = {Automatically finding patches using genetic programming},
  Author                   = {Weimer, Westley and Nguyen, ThanhVu and Le Goues, Claire and Forrest, Stephanie},
  Booktitle                = {Proceedings of the 31st International Conference on Software Engineering(ICSE'09)},
  Year                     = {2009},

  Address                  = {Washington, DC, USA},
  Pages                    = {364--374},
  Publisher                = {IEEE Computer Society},
  Series                   = {ICSE'09},

  Acmid                    = {1555051},
  Doi                      = {10.1109/ICSE.2009.5070536},
  ISBN                     = {978-1-4244-3453-4},
  Numpages                 = {11},
  Owner                    = {drg},
  Timestamp                = {2012.09.15},
  Url                      = {http://dx.doi.org/10.1109/ICSE.2009.5070536}
}

@Article{Wen12_SLREffEst,
  Title                    = {Systematic literature review of machine learning based software development effort estimation models},
  Author                   = {Jianfeng Wen and Shixian Li and Zhiyong Lin and Yong Hu and Changqin Huang},
  Journal                  = {Information and Software Technology},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {41--59},
  Volume                   = {54},

  Abstract                 = {Context Software development effort estimation (SDEE) is the process of predicting the effort required to develop a software system. In order to improve estimation accuracy, many researchers have proposed machine learning (ML) based SDEE models (ML models) since 1990s. However, there has been no attempt to analyze the empirical evidence on ML models in a systematic way. Objective This research aims to systematically analyze ML models from four aspects: type of ML technique, estimation accuracy, model comparison, and estimation context. Method We performed a systematic literature review of empirical studies on ML model published in the last two decades (1991-2010). Results We have identified 84 primary studies relevant to the objective of this research. After investigating these studies, we found that eight types of ML techniques have been employed in SDEE models. Overall speaking, the estimation accuracy of these ML models is close to the acceptable level and is better than that of non-ML models. Furthermore, different ML models have different strengths and weaknesses and thus favor different estimation contexts. Conclusion ML models are promising in the field of SDEE. However, the application of ML models in industry is still limited, so that more effort and incentives are needed to facilitate the application of ML models. To this end, based on the findings of this review, we provide recommendations for researchers as well as guidelines for practitioners.},
  Doi                      = {10.1016/j.infsof.2011.09.002},
  ISSN                     = {0950-5849},
  Keywords                 = {Software effort estimation},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950584911001832}
}

@Article{Williams09,
  Title                    = {Rattle: A Data Mining GUI for R},
  Author                   = {G. Williams},
  Journal                  = {The R Journal},
  Year                     = {2009},
  Number                   = {2},
  Pages                    = {45--55},
  Volume                   = {2}
}

@Article{Woodfield81,
  Title                    = {A study of several metrics for programming effort},
  Author                   = {S.N. Woodfield and V.Y. Shen and H.E. Dunsmore},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1981},
  Number                   = {2},
  Pages                    = {97--103},
  Volume                   = {2},

  Abstract                 = {As the cost of programming becomes a major component of the cost of computer systems, it becomes imperative that program development and maintenance be better managed. One measurement a manager could use is programming complexity. Such a measure can be very useful if the manager is confident that the higher the complexity measure is for a programming project, the more effort it takes to complete the project and perhaps to maintain it. Until recently most measures of complexity were based only on intuition and experience. In the past 3 years two objective metrics have been introduced, McCabe's cyclomatic number v(G) and Halstead's effort measure E. This paper reports an empirical study designed to compare these two metrics with a classic size measure, lines of code. A fourth metric based on a model of programming is introduced and shown to be better than the previously known metrics for some experimental data.},
  Doi                      = {10.1016/0164-1212(81)90029-7},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0164121281900297}
}

@Article{4407730,
  Title                    = {On the Distribution of Software Faults},
  Author                   = {Hongyu Zhang},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2008},

  Month                    = {march-april },
  Number                   = {2},
  Pages                    = {301--302},
  Volume                   = {34},

  Doi                      = {10.1109/TSE.2007.70771},
  ISSN                     = {0098-5589},
  Keywords                 = {Pareto principle;Weibull distribution;large software systems;software faults distribution;Pareto analysis;Weibull distribution;}
}

@Article{ZhangHFM11,
  Title                    = {Comparing the Performance of Metaheuristics for the Analysis of Multi-stakeholder Tradeoffs in Requirements Optimisation},
  Author                   = {Yuanyuan Zhang and Mark Harman and Anthony Finkelstein and S. Afshin Mansouri},
  Journal                  = {Information and Software Technology},
  Year                     = {2011},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {761-773},
  Volume                   = {53},

  Doi                      = {http://dx.doi.org/10.1016/j.infsof.2011.02.001},
  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@InProceedings{ZPZ07,
  Title                    = {Predicting Defects for Eclipse},
  Author                   = {Zimmermann, Thomas and Premraj, Rahul and Zeller, Andreas},
  Booktitle                = {Proceedings of the Third International Workshop on Predictor Models in Software Engineering (PROMISE'07)},
  Year                     = {2007},

  Address                  = {Washington, DC, USA},
  Pages                    = {9--},
  Publisher                = {IEEE Computer Society},
  Series                   = {PROMISE '07},

  Acmid                    = {1269057},
  Doi                      = {http://dx.doi.org/10.1109/PROMISE.2007.10},
  ISBN                     = {0-7695-2954-2}
}

